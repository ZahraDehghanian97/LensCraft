{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZahraDehghanian97/LenseCraft/blob/master/Cinematography_Instruction_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI_pTggHYrwl"
      },
      "source": [
        "#Imports and Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlSRuyBKY64_"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqHtkNVtxah8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "from enum import Enum\n",
        "from typing import List, Dict\n",
        "import random\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpUEXi1jxsMW",
        "outputId": "a68e4510-dd08-42ee-ecc4-15af806d813a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1VT2XfBj9LFWLUBjv65dzC4bVzH0zdNDU\n",
            "From (redirected): https://drive.google.com/uc?id=1VT2XfBj9LFWLUBjv65dzC4bVzH0zdNDU&confirm=t&uuid=1e4c29b9-0ef5-4191-8e9a-ad1390da2b81\n",
            "To: /content/random_simulation_dataset.json\n",
            "100% 105M/105M [00:00<00:00, 146MB/s] \n"
          ]
        }
      ],
      "source": [
        "!gdown -O /content/random_simulation_dataset.json 1VT2XfBj9LFWLUBjv65dzC4bVzH0zdNDU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbuy_qhbY9_D"
      },
      "source": [
        "##Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR_z1bzx1YRk"
      },
      "outputs": [],
      "source": [
        "class CameraMovementType(Enum):\n",
        "    panLeft = \"panLeft\"\n",
        "    panRight = \"panRight\"\n",
        "    tiltUp = \"tiltUp\"\n",
        "    tiltDown = \"tiltDown\"\n",
        "    dollyIn = \"dollyIn\"\n",
        "    dollyOut = \"dollyOut\"\n",
        "    truckLeft = \"truckLeft\"\n",
        "    truckRight = \"truckRight\"\n",
        "    pedestalUp = \"pedestalUp\"\n",
        "    pedestalDown = \"pedestalDown\"\n",
        "    fullZoomIn = \"fullZoomIn\"\n",
        "    fullZoomOut = \"fullZoomOut\"\n",
        "    halfZoomIn = \"halfZoomIn\"\n",
        "    halfZoomOut = \"halfZoomOut\"\n",
        "    shortZoomIn = \"shortZoomIn\"\n",
        "    shortZoomOut = \"shortZoomOut\"\n",
        "    shortArcShotLeft = \"shortArcShotLeft\"\n",
        "    shortArcShotRight = \"shortArcShotRight\"\n",
        "    halfArcShotLeft = \"halfArcShotLeft\"\n",
        "    halfArcShotRight = \"halfArcShotRight\"\n",
        "    fullArcShotLeft = \"fullArcShotLeft\"\n",
        "    fullArcShotRight = \"fullArcShotRight\"\n",
        "    panAndTilt = \"panAndTilt\"\n",
        "    dollyAndPan = \"dollyAndPan\"\n",
        "    zoomAndTruck = \"zoomAndTruck\"\n",
        "\n",
        "class EasingType(Enum):\n",
        "    linear = \"linear\"\n",
        "    easeInQuad = \"easeInQuad\"\n",
        "    easeInCubic = \"easeInCubic\"\n",
        "    easeInQuart = \"easeInQuart\"\n",
        "    easeInQuint = \"easeInQuint\"\n",
        "    easeOutQuad = \"easeOutQuad\"\n",
        "    easeOutCubic = \"easeOutCubic\"\n",
        "    easeOutQuart = \"easeOutQuart\"\n",
        "    easeOutQuint = \"easeOutQuint\"\n",
        "    easeInOutQuad = \"easeInOutQuad\"\n",
        "    easeInOutCubic = \"easeInOutCubic\"\n",
        "    easeInOutQuart = \"easeInOutQuart\"\n",
        "    easeInOutQuint = \"easeInOutQuint\"\n",
        "    easeInSine = \"easeInSine\"\n",
        "    easeOutSine = \"easeOutSine\"\n",
        "    easeInOutSine = \"easeInOutSine\"\n",
        "    easeInExpo = \"easeInExpo\"\n",
        "    easeOutExpo = \"easeOutExpo\"\n",
        "    easeInOutExpo = \"easeInOutExpo\"\n",
        "    easeInCirc = \"easeInCirc\"\n",
        "    easeOutCirc = \"easeOutCirc\"\n",
        "    easeInOutCirc = \"easeInOutCirc\"\n",
        "    easeInBounce = \"easeInBounce\"\n",
        "    easeOutBounce = \"easeOutBounce\"\n",
        "    easeInOutBounce = \"easeInOutBounce\"\n",
        "    easeInElastic = \"easeInElastic\"\n",
        "    easeOutElastic = \"easeOutElastic\"\n",
        "    easeInOutElastic = \"easeInOutElastic\"\n",
        "\n",
        "class CameraAngle(Enum):\n",
        "    lowAngle = \"lowAngle\"\n",
        "    mediumAngle = \"mediumAngle\"\n",
        "    highAngle = \"highAngle\"\n",
        "    birdsEyeView = \"birdsEyeView\"\n",
        "\n",
        "class ShotType(Enum):\n",
        "    closeUp = \"closeUp\"\n",
        "    mediumShot = \"mediumShot\"\n",
        "    longShot = \"longShot\"\n",
        "\n",
        "\n",
        "movement_descriptions = {\n",
        "    \"panLeft\": \"panning left\",\n",
        "    \"panRight\": \"panning right\",\n",
        "    \"tiltUp\": \"tilting up\",\n",
        "    \"tiltDown\": \"tilting down\",\n",
        "    \"dollyIn\": \"moving closer\",\n",
        "    \"dollyOut\": \"moving away\",\n",
        "    \"truckLeft\": \"moving left\",\n",
        "    \"truckRight\": \"moving right\",\n",
        "    \"pedestalUp\": \"rising vertically\",\n",
        "    \"pedestalDown\": \"descending vertically\",\n",
        "    \"fullZoomIn\": \"zooming in fully\",\n",
        "    \"fullZoomOut\": \"zooming out fully\",\n",
        "    \"halfZoomIn\": \"zooming in halfway\",\n",
        "    \"halfZoomOut\": \"zooming out halfway\",\n",
        "    \"shortZoomIn\": \"zooming in slightly\",\n",
        "    \"shortZoomOut\": \"zooming out slightly\",\n",
        "    \"shortArcShotLeft\": \"moving in a short arc to the left\",\n",
        "    \"shortArcShotRight\": \"moving in a short arc to the right\",\n",
        "    \"halfArcShotLeft\": \"moving in a half arc to the left\",\n",
        "    \"halfArcShotRight\": \"moving in a half arc to the right\",\n",
        "    \"fullArcShotLeft\": \"moving in a full arc to the left\",\n",
        "    \"fullArcShotRight\": \"moving in a full arc to the right\",\n",
        "    \"panAndTilt\": \"panning and tilting\",\n",
        "    \"dollyAndPan\": \"moving and panning\",\n",
        "    \"zoomAndTruck\": \"zooming and moving sideways\",\n",
        "}\n",
        "\n",
        "easing_descriptions = {\n",
        "    \"linear\": \"at a constant speed\",\n",
        "    \"easeInQuad\": \"slowly at first, then accelerating gradually\",\n",
        "    \"easeInCubic\": \"slowly at first, then accelerating more rapidly\",\n",
        "    \"easeInQuart\": \"very slowly at first, then accelerating dramatically\",\n",
        "    \"easeInQuint\": \"extremely slowly at first, then accelerating very dramatically\",\n",
        "    \"easeOutQuad\": \"quickly at first, then decelerating gradually\",\n",
        "    \"easeOutCubic\": \"quickly at first, then decelerating more rapidly\",\n",
        "    \"easeOutQuart\": \"very quickly at first, then decelerating dramatically\",\n",
        "    \"easeOutQuint\": \"extremely quickly at first, then decelerating very dramatically\",\n",
        "    \"easeInOutQuad\": \"gradually accelerating, then gradually decelerating\",\n",
        "    \"easeInOutCubic\": \"slowly accelerating, then decelerating more rapidly\",\n",
        "    \"easeInOutQuart\": \"slowly accelerating, then decelerating dramatically\",\n",
        "    \"easeInOutQuint\": \"very slowly accelerating, then decelerating very dramatically\",\n",
        "    \"easeInSine\": \"with a gentle start, gradually increasing in speed\",\n",
        "    \"easeOutSine\": \"quickly at first, then gently decelerating\",\n",
        "    \"easeInOutSine\": \"with a gentle start and end, faster in the middle\",\n",
        "    \"easeInExpo\": \"starting very slowly, then accelerating exponentially\",\n",
        "    \"easeOutExpo\": \"starting very fast, then decelerating exponentially\",\n",
        "    \"easeInOutExpo\": \"starting and ending slowly, with rapid acceleration and deceleration in the middle\",\n",
        "    \"easeInCirc\": \"starting slowly, then accelerating sharply towards the end\",\n",
        "    \"easeOutCirc\": \"starting quickly, then decelerating sharply towards the end\",\n",
        "    \"easeInOutCirc\": \"with sharp acceleration and deceleration at both ends\",\n",
        "    \"easeInBounce\": \"with a bouncing effect that intensifies towards the end\",\n",
        "    \"easeOutBounce\": \"quickly at first, then bouncing to a stop\",\n",
        "    \"easeInOutBounce\": \"with a bouncing effect at both the start and end\",\n",
        "    \"easeInElastic\": \"with an elastic effect that intensifies towards the end\",\n",
        "    \"easeOutElastic\": \"quickly at first, then oscillating to a stop\",\n",
        "    \"easeInOutElastic\": \"with an elastic effect at both the start and end\",\n",
        "}\n",
        "\n",
        "angle_descriptions = {\n",
        "    \"lowAngle\": \"from a low angle\",\n",
        "    \"mediumAngle\": \"from a medium angle\",\n",
        "    \"highAngle\": \"from a high angle\",\n",
        "    \"birdsEyeView\": \"from a bird's eye view\",\n",
        "}\n",
        "\n",
        "shot_descriptions = {\n",
        "    \"closeUp\": \"in a close-up shot\",\n",
        "    \"mediumShot\": \"in a medium shot\",\n",
        "    \"longShot\": \"in a long shot\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT895usiy-v9"
      },
      "outputs": [],
      "source": [
        "def get_clip_embedding(text: str) -> torch.Tensor:\n",
        "    with torch.no_grad():\n",
        "        inputs = clip_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        embedding = clip_text_encoder(**inputs).pooler_output\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKcsSyQYy6AN"
      },
      "outputs": [],
      "source": [
        "def extract_camera_frame_data(camera_frames):\n",
        "    return [\n",
        "        [\n",
        "            frame['position']['x'],\n",
        "            frame['position']['y'],\n",
        "            frame['position']['z'],\n",
        "            frame['focalLength'],\n",
        "            frame['angle']['x'],\n",
        "            frame['angle']['y'],\n",
        "            frame['angle']['z']\n",
        "        ]\n",
        "        for frame in camera_frames\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def circular_distance_loss(pred, target):\n",
        "    pred = torch.clamp(pred, min=-np.pi, max=np.pi)\n",
        "    target = torch.clamp(target, min=-np.pi, max=np.pi)\n",
        "    \n",
        "    distance = torch.abs(pred - target)\n",
        "    distance = torch.where(distance > np.pi, 2 * np.pi - distance, distance)\n",
        "    \n",
        "    return torch.mean(distance ** 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_decay(initial_value, final_value, current_epoch, total_epochs):\n",
        "    cosine_decay = 0.5 * (1 + math.cos(math.pi * current_epoch / total_epochs))\n",
        "    return final_value + (initial_value - final_value) * cosine_decay\n",
        "\n",
        "def linear_increase(initial_value, final_value, current_epoch, total_epochs):\n",
        "    return initial_value + (final_value - initial_value) * (current_epoch / total_epochs)\n",
        "\n",
        "def get_noise_and_mask_values(current_epoch, total_epochs, config):\n",
        "    noise_std = cosine_decay(\n",
        "        initial_value=config['initial_noise_std'],\n",
        "        final_value=config['final_noise_std'],\n",
        "        current_epoch=current_epoch,\n",
        "        total_epochs=total_epochs\n",
        "    )\n",
        "\n",
        "    mask_ratio = linear_increase(\n",
        "        initial_value=config['initial_mask_ratio'],\n",
        "        final_value=config['final_mask_ratio'],\n",
        "        current_epoch=current_epoch,\n",
        "        total_epochs=total_epochs\n",
        "    )\n",
        "\n",
        "    return noise_std, mask_ratio\n",
        "\n",
        "def apply_mask_and_noise(data, mask_ratio=0.0, noise_std=0.0):\n",
        "    mask = torch.bernoulli(torch.full((data.shape[0], data.shape[1]), 1 - mask_ratio, device=device)).bool()\n",
        "\n",
        "    masked_data = data.clone()\n",
        "    masked_data[~mask] = 0\n",
        "\n",
        "    noisy_data = masked_data + torch.normal(mean=0, std=noise_std, size=data.shape, device=device)\n",
        "\n",
        "    src_key_padding_mask = ~mask\n",
        "\n",
        "    return noisy_data, mask, src_key_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_random_instruction():\n",
        "    return {\n",
        "        'movement': random.choice(list(CameraMovementType)),\n",
        "        'easing': random.choice(list(EasingType)),\n",
        "        'camera_angle': random.choice(list(CameraAngle)),\n",
        "        'shot_type': random.choice(list(ShotType))\n",
        "    }\n",
        "\n",
        "def instruction_to_latent(instruction, model):\n",
        "    movement_clip = movement_clip_dict[instruction['movement'].value].unsqueeze(0)\n",
        "    easing_clip = easing_clip_dict[instruction['easing'].value].unsqueeze(0)\n",
        "    angle_clip = angle_clip_dict[instruction['camera_angle'].value].unsqueeze(0)\n",
        "    shot_clip = shot_clip_dict[instruction['shot_type'].value].unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        latent = model.merge_latents(movement_clip, easing_clip, angle_clip, shot_clip)\n",
        "\n",
        "    return latent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u621kky1Zm8u"
      },
      "outputs": [],
      "source": [
        "def process_camera_trajectory(trajectory: torch.Tensor) -> List[Dict]:\n",
        "    trajectory = trajectory.cpu().detach().numpy()\n",
        "    camera_frames = []\n",
        "    for frame in trajectory:\n",
        "        position, angle, focal_length = frame[:3], frame[3:6], frame[6]\n",
        "        camera_frame = {\n",
        "            \"position\": {axis: float(value) for axis, value in zip(['x', 'y', 'z'], position)},\n",
        "            \"angle\": {axis: float(value) for axis, value in zip(['x', 'y', 'z'], angle)},\n",
        "            \"focalLength\": float(focal_length)\n",
        "        }\n",
        "        camera_frames.append(camera_frame)\n",
        "    return camera_frames\n",
        "\n",
        "def save_to_json(trajectory: torch.Tensor, filename: str):\n",
        "    camera_frames = process_camera_trajectory(trajectory)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(camera_frames, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsJo1-aVZaSK"
      },
      "source": [
        "#Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU3OzBiKt2dt",
        "outputId": "790f5429-9479-406f-9b93-ecb760801966"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "clip_model_name = \"openai/clip-vit-large-patch14\" #@param [\"openai/clip-vit-base-patch32\", \"openai/clip-vit-large-patch14\"]\n",
        "\n",
        "if clip_model_name == \"openai/clip-vit-large-patch14\":\n",
        "    clip_embedding_dim = 768\n",
        "else:\n",
        "    clip_embedding_dim = 512\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
        "clip_text_encoder = CLIPTextModel.from_pretrained(clip_model_name).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGc3_s9r1h9s",
        "outputId": "8ad82ca7-db25-41e0-be4f-ccb7595ce4aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing batches: 100%|██████████| 140/140 [00:29<00:00,  4.73it/s]\n"
          ]
        }
      ],
      "source": [
        "movement_clip_embeddings = get_clip_embedding(list(movement_descriptions.values()))\n",
        "easing_clip_embeddings = get_clip_embedding(list(easing_descriptions.values()))\n",
        "angle_clip_embeddings = get_clip_embedding(list(angle_descriptions.values()))\n",
        "shot_clip_embeddings = get_clip_embedding(list(shot_descriptions.values()))\n",
        "\n",
        "movement_clip_dict = {k: movement_clip_embeddings[i] for i, k in enumerate(movement_descriptions.keys())}\n",
        "easing_clip_dict = {k: easing_clip_embeddings[i] for i, k in enumerate(easing_descriptions.keys())}\n",
        "angle_clip_dict = {k: angle_clip_embeddings[i] for i, k in enumerate(angle_descriptions.keys())}\n",
        "shot_clip_dict = {k: shot_clip_embeddings[i] for i, k in enumerate(shot_descriptions.keys())}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yArArZxWxMMj"
      },
      "outputs": [],
      "source": [
        "class SimulationDataset(Dataset):\n",
        "    def __init__(self, json_file_path: str):\n",
        "        with open(json_file_path, 'r') as file:\n",
        "            raw_data = json.load(file)\n",
        "        self.simulation_data = [self._process_single_simulation(sim) for sim in raw_data['simulations']\n",
        "                if self._is_simulation_valid(sim)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.simulation_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.simulation_data[index]\n",
        "\n",
        "    def _is_simulation_valid(self, simulation):\n",
        "        return (len(simulation['instructions']) == 1 and\n",
        "                simulation['instructions'][0]['frameCount'] == 30 and\n",
        "                len(simulation['cameraFrames']) == 30)\n",
        "\n",
        "    def _process_single_simulation(self, simulation):\n",
        "        instruction = simulation['instructions'][0]\n",
        "        subject = simulation['subjects'][0]\n",
        "\n",
        "        camera_trajectory = extract_camera_frame_data(simulation['cameraFrames'])\n",
        "\n",
        "        movement_type = CameraMovementType[instruction['cameraMovement']]\n",
        "        easing_type = EasingType[instruction['movementEasing']]\n",
        "        camera_angle = CameraAngle[instruction.get('initialCameraAngle', 'mediumAngle')]\n",
        "        shot_type = ShotType[instruction.get('initialShotType', 'mediumShot')]\n",
        "\n",
        "        subject_data = [\n",
        "            subject['position']['x'], subject['position']['y'], subject['position']['z'],\n",
        "            subject['size']['x'], subject['size']['y'], subject['size']['z'],\n",
        "            subject['rotation']['x'], subject['rotation']['y'], subject['rotation']['z']\n",
        "        ]\n",
        "\n",
        "        return {\n",
        "            'camera_trajectory': torch.tensor(camera_trajectory, dtype=torch.float32),\n",
        "            'subject': torch.tensor(subject_data, dtype=torch.float32),\n",
        "            'movement_type': movement_type,\n",
        "            'easing_type': easing_type,\n",
        "            'camera_angle': camera_angle,\n",
        "            'shot_type': shot_type\n",
        "        }\n",
        "\n",
        "def batch_collate(batch):\n",
        "    return {\n",
        "        'camera_trajectory': torch.stack([item['camera_trajectory'] for item in batch]),\n",
        "        'subject': torch.stack([item['subject'] for item in batch]),\n",
        "        'movement_type': [item['movement_type'] for item in batch],\n",
        "        'easing_type': [item['easing_type'] for item in batch],\n",
        "        'camera_angle': [item['camera_angle'] for item in batch],\n",
        "        'shot_type': [item['shot_type'] for item in batch]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu60D7QlzDJR"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, latent_dim, nhead, num_encoder_layers, dim_feedforward, dropout_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.input_projection = nn.Linear(input_dim, latent_dim)\n",
        "        self.pos_encoder = PositionalEncoding(latent_dim)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=latent_dim, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout_rate)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        self.query_tokens = nn.ParameterDict({\n",
        "            f\"{qt}_query\": nn.Parameter(torch.randn(1, 1, latent_dim))\n",
        "            for qt in ['movement', 'easing', 'camera_angle', 'shot_type']\n",
        "        })\n",
        "\n",
        "    def forward(self, src, subject_embedded, src_key_padding_mask=None):\n",
        "        src_embedded = self.input_projection(src)\n",
        "        src_embedded = torch.cat([subject_embedded, src_embedded], dim=1)\n",
        "        src_embedded = self.pos_encoder(src_embedded)\n",
        "        src_embedded = src_embedded.permute(1, 0, 2)\n",
        "\n",
        "        query_tokens = torch.cat([query.repeat(1, src_embedded.shape[1], 1) for query in self.query_tokens.values()], dim=0)\n",
        "        src_with_queries = torch.cat([query_tokens, src_embedded], dim=0)\n",
        "\n",
        "        if src_key_padding_mask is not None:\n",
        "            subject_mask = torch.zeros((src_key_padding_mask.shape[0], 1), dtype=torch.bool, device=device)\n",
        "            src_key_padding_mask = torch.cat([subject_mask, src_key_padding_mask], dim=1)\n",
        "            query_mask = torch.zeros((src_key_padding_mask.shape[0], len(self.query_tokens)), dtype=torch.bool, device=device)\n",
        "            src_key_padding_mask = torch.cat([query_mask, src_key_padding_mask], dim=1)\n",
        "\n",
        "        memory = self.transformer_encoder(src_with_queries, src_key_padding_mask=src_key_padding_mask)\n",
        "\n",
        "        return memory[:len(self.query_tokens)]\n",
        "\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, latent_dim, nhead, num_decoder_layers, dim_feedforward, dropout_rate, seq_length):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.pos_encoder = PositionalEncoding(latent_dim)\n",
        "        self.embedding = nn.Linear(output_dim, latent_dim)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=latent_dim, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout_rate)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.output_projection = nn.Linear(latent_dim, output_dim)\n",
        "\n",
        "    def forward(self, memory, decoder_input, subject_embedded, tgt_mask):\n",
        "        embedded = self.embedding(decoder_input)\n",
        "        embedded = torch.cat([subject_embedded, embedded], dim=1)\n",
        "        embedded = self.pos_encoder(embedded)\n",
        "        embedded = embedded.transpose(0, 1)\n",
        "\n",
        "        output = self.transformer_decoder(embedded, memory, tgt_mask=tgt_mask)\n",
        "        output = output.transpose(0, 1)\n",
        "        output = self.output_projection(output[:, 1:, :])  # Remove subject from output\n",
        "\n",
        "        return output\n",
        "\n",
        "class MultiTaskAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim=7, subject_dim=9, nhead=4, num_encoder_layers=3, num_decoder_layers=3, dim_feedforward=2048,\n",
        "                 dropout_rate=0.1, seq_length=30, latent_dim=512):\n",
        "        super(MultiTaskAutoencoder, self).__init__()\n",
        "\n",
        "        self.subject_projection = nn.Linear(subject_dim, latent_dim)\n",
        "        self.encoder = Encoder(input_dim, latent_dim, nhead, num_encoder_layers, dim_feedforward, dropout_rate)\n",
        "        self.decoder = Decoder(input_dim, latent_dim, nhead, num_decoder_layers, dim_feedforward, dropout_rate, seq_length)\n",
        "\n",
        "        self.latent_merger = nn.Linear(latent_dim * 4, latent_dim)\n",
        "\n",
        "        self.seq_length = seq_length\n",
        "        self.input_dim = input_dim\n",
        "\n",
        "    def generate_square_subsequent_mask(self, sz):\n",
        "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "        return mask\n",
        "\n",
        "    def autoregressive_decode(self, latent, subject_embedded, target=None, teacher_forcing_ratio=0.5):\n",
        "        memory = latent.unsqueeze(1).repeat(1, self.seq_length, 1)\n",
        "        memory = memory.transpose(0, 1)\n",
        "\n",
        "        decoder_input = torch.zeros(latent.shape[0], 1, self.input_dim, device=device)\n",
        "        outputs = []\n",
        "\n",
        "        for t in range(self.seq_length):\n",
        "            tgt_mask = self.generate_square_subsequent_mask(t + 2).to(device)\n",
        "            \n",
        "            output = self.decoder(memory, decoder_input, subject_embedded, tgt_mask)\n",
        "            outputs.append(output[:, -1:, :])\n",
        "\n",
        "            if target is not None and torch.rand(1).item() < teacher_forcing_ratio:\n",
        "                decoder_input = torch.cat([decoder_input, target[:, t:t+1, :]], dim=1)\n",
        "            else:\n",
        "                decoder_input = torch.cat([decoder_input, output[:, -1:, :]], dim=1)\n",
        "\n",
        "        return torch.cat(outputs, dim=1)\n",
        "\n",
        "    def merge_latents(self, movement_embedding, easing_embedding, camera_angle_embedding, shot_type_embedding):\n",
        "        combined = torch.cat([movement_embedding, easing_embedding, camera_angle_embedding, shot_type_embedding], dim=-1)\n",
        "        return self.latent_merger(combined)\n",
        "\n",
        "    def forward(self, src, subject, src_key_padding_mask=None, target=None, teacher_forcing_ratio=0.5):\n",
        "        subject_embedded = self.subject_projection(subject).unsqueeze(1)\n",
        "        movement_embedding, easing_embedding, camera_angle_embedding, shot_type_embedding = self.encoder(src, subject_embedded, src_key_padding_mask)\n",
        "        latent = self.merge_latents(movement_embedding, easing_embedding, camera_angle_embedding, shot_type_embedding)\n",
        "        reconstructed = self.autoregressive_decode(latent, subject_embedded, target, teacher_forcing_ratio)\n",
        "\n",
        "        return {\n",
        "            'movement_embedding': movement_embedding,\n",
        "            'easing_embedding': easing_embedding,\n",
        "            'camera_angle_embedding': camera_angle_embedding,\n",
        "            'shot_type_embedding': shot_type_embedding,\n",
        "            'reconstructed': reconstructed,\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2OsT98BzXKy"
      },
      "outputs": [],
      "source": [
        "def init_losses():\n",
        "    return {\n",
        "        'total': 0,\n",
        "        'reconstruction': 0,\n",
        "        'clip_movement': 0,\n",
        "        'clip_easing': 0,\n",
        "        'clip_camera_angle': 0,\n",
        "        'clip_shot_type': 0\n",
        "    }\n",
        "\n",
        "def print_detailed_losses(phase, losses):\n",
        "    print(f\"{phase} Losses:\", end=' ')\n",
        "    for key, value in losses.items():\n",
        "        print(f\"  {key}: {value:.4f}\", end=' ')\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eglTw5WODOIO"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion, noise_std=0.0, mask_ratio=0.0, teacher_forcing_ratio=0.5):\n",
        "    model.train()\n",
        "    total_losses = init_losses()\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        losses = process_batch(model, batch, criterion, noise_std, mask_ratio, teacher_forcing_ratio)\n",
        "\n",
        "        loss = losses['total']\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for key in total_losses:\n",
        "            total_losses[key] += losses[key].item() if torch.is_tensor(losses[key]) else losses[key]\n",
        "\n",
        "    return {k: v / len(dataloader) for k, v in total_losses.items()}\n",
        "\n",
        "def validate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_losses = init_losses()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
        "            losses = process_batch(model, batch, criterion, teacher_forcing_ratio=0)\n",
        "\n",
        "            for key in total_losses:\n",
        "                total_losses[key] += losses[key].item() if torch.is_tensor(losses[key]) else losses[key]\n",
        "\n",
        "    return {k: v / len(dataloader) for k, v in total_losses.items()}\n",
        "\n",
        "\n",
        "def process_batch(model, batch, criterion, noise_std=0.0, mask_ratio=0.0, teacher_forcing_ratio=0.5):\n",
        "    camera_trajectory = batch['camera_trajectory'].to(device)\n",
        "    subject = batch['subject'].to(device)\n",
        "    movement_clip = torch.stack([movement_clip_dict[t.value] for t in batch['movement_type']]).to(device)\n",
        "    easing_clip = torch.stack([easing_clip_dict[t.value] for t in batch['easing_type']]).to(device)\n",
        "    angle_clip = torch.stack([angle_clip_dict[t.value] for t in batch['camera_angle']]).to(device)\n",
        "    shot_clip = torch.stack([shot_clip_dict[t.value] for t in batch['shot_type']]).to(device)\n",
        "\n",
        "    noisy_trajectory, mask, src_key_padding_mask = apply_mask_and_noise(camera_trajectory, mask_ratio, noise_std)\n",
        "\n",
        "    target = camera_trajectory if model.training else None\n",
        "\n",
        "    output = model(noisy_trajectory, subject, src_key_padding_mask, target, teacher_forcing_ratio)\n",
        "\n",
        "    masked_output = output['reconstructed'][mask]\n",
        "    masked_target = camera_trajectory[mask]\n",
        "\n",
        "    position_output = masked_output[:, :5]\n",
        "    position_target = masked_target[:, :5]\n",
        "    angle_output = masked_output[:, 5:]\n",
        "    angle_target = masked_target[:, 5:]\n",
        "\n",
        "    position_loss = criterion['reconstruction'](position_output, position_target)\n",
        "    angle_loss = circular_distance_loss(angle_output, angle_target)\n",
        "\n",
        "    losses = {\n",
        "        'reconstruction': position_loss + angle_loss,\n",
        "        'clip_movement': 1 - F.cosine_similarity(output['movement_embedding'], movement_clip).mean(),\n",
        "        'clip_easing': 1 - F.cosine_similarity(output['easing_embedding'], easing_clip).mean(),\n",
        "        'clip_camera_angle': 1 - F.cosine_similarity(output['camera_angle_embedding'], angle_clip).mean(),\n",
        "        'clip_shot_type': 1 - F.cosine_similarity(output['shot_type_embedding'], shot_clip).mean(),\n",
        "    }\n",
        "    losses['total'] = sum(losses.values())\n",
        "\n",
        "    return losses\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, config):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
        "    criterion = {\n",
        "        'reconstruction': nn.MSELoss()\n",
        "    }\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        current_teacher_forcing_ratio = config['init_teacher_forcing_ratio'] * (1 - epoch / config['num_epochs'])\n",
        "        current_noise_std, current_mask_ratio = get_noise_and_mask_values(epoch, config['num_epochs'], config)\n",
        "\n",
        "        train_losses = train_epoch(model, train_dataloader, optimizer, criterion,\n",
        "                                   current_noise_std, current_mask_ratio, current_teacher_forcing_ratio)\n",
        "        val_losses = validate(model, val_dataloader, criterion)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
        "        print(f\"Train Loss: {train_losses['total']:.4f}, Validation Loss: {val_losses['total']:.4f}\")\n",
        "        print_detailed_losses(\"Train\", train_losses)\n",
        "        print_detailed_losses(\"Valid\", val_losses)\n",
        "\n",
        "        if val_losses['total'] < best_val_loss:\n",
        "            best_val_loss = val_losses['total']\n",
        "            epochs_without_improvement = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(\"New best model saved!\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        if epochs_without_improvement >= config['patience']:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GPK8Tp53m8d"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset = SimulationDataset('random_simulation_dataset.json')\n",
        "\n",
        "train_dataset, val_dataset = train_test_split(dataset, test_size=0.3, random_state=42)\n",
        "# train_dataset = val_dataset = [dataset[0]] * 32\n",
        "\n",
        "batch_size = 32\n",
        "num_workers = 2\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                              collate_fn=batch_collate, num_workers=num_workers)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                            collate_fn=batch_collate, num_workers=num_workers)\n",
        "\n",
        "model = MultiTaskAutoencoder(latent_dim=clip_embedding_dim, dropout_rate=0.2).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8zCiaVe3ltM"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'num_epochs': 2000,\n",
        "    'patience': 30,\n",
        "    'learning_rate': 0.0001,\n",
        "    'weight_decay': 1e-5,\n",
        "    'initial_noise_std': 0.2,\n",
        "    'final_noise_std': 0.05,\n",
        "    'initial_mask_ratio': 0.3,\n",
        "    'final_mask_ratio': 0.7,\n",
        "    'init_teacher_forcing_ratio': 0.8\n",
        "}\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "train_model(model, train_dataloader, val_dataloader, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONMw8qWQkKqN"
      },
      "source": [
        "##Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp5tXDnz02Uj"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Il2pEix4xQEv"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/result.json'\n",
        "\n",
        "# random_instruction = generate_random_instruction()\n",
        "# print(\"Random Instruction:\", random_instruction)\n",
        "instruction = {\n",
        "    'movement': CameraMovementType['dollyIn'],\n",
        "    'easing': EasingType['easeInQuad'],\n",
        "    'camera_angle': CameraAngle['highAngle'],\n",
        "    'shot_type': ShotType['longShot']\n",
        "}\n",
        "\n",
        "latent = instruction_to_latent(instruction, model)\n",
        "with torch.no_grad():\n",
        "    camera_frames = model.autoregressive_decode(latent).squeeze(0)\n",
        "save_to_json(camera_frames, file_path)\n",
        "\n",
        "files.download(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEKo1EUbn25b"
      },
      "outputs": [],
      "source": [
        "input_path = '/content/input.json'\n",
        "output_path = '/content/output.json'\n",
        "\n",
        "input_camera_tranjectory = dataset[0]['camera_trajectory'].to(device)\n",
        "reconstructed_camera_tranjectory = model(input_camera_tranjectory.unsqueeze(0))['reconstructed'].squeeze(0)\n",
        "\n",
        "save_to_json(input_camera_tranjectory, input_path)\n",
        "save_to_json(reconstructed_camera_tranjectory, output_path)\n",
        "\n",
        "files.download(input_path)\n",
        "files.download(output_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
