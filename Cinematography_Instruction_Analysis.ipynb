{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZahraDehghanian97/LenseCraft/blob/master/Cinematography_Instruction_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI_pTggHYrwl"
      },
      "source": [
        "#Imports and Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlSRuyBKY64_"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uqHtkNVtxah8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "from enum import Enum\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rpUEXi1jxsMW"
      },
      "outputs": [],
      "source": [
        "!gdown 1VT2XfBj9LFWLUBjv65dzC4bVzH0zdNDU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbuy_qhbY9_D"
      },
      "source": [
        "##Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gR_z1bzx1YRk"
      },
      "outputs": [],
      "source": [
        "class CameraMovementType(Enum):\n",
        "    panLeft = \"panLeft\"\n",
        "    panRight = \"panRight\"\n",
        "    tiltUp = \"tiltUp\"\n",
        "    tiltDown = \"tiltDown\"\n",
        "    dollyIn = \"dollyIn\"\n",
        "    dollyOut = \"dollyOut\"\n",
        "    truckLeft = \"truckLeft\"\n",
        "    truckRight = \"truckRight\"\n",
        "    pedestalUp = \"pedestalUp\"\n",
        "    pedestalDown = \"pedestalDown\"\n",
        "    fullZoomIn = \"fullZoomIn\"\n",
        "    fullZoomOut = \"fullZoomOut\"\n",
        "    halfZoomIn = \"halfZoomIn\"\n",
        "    halfZoomOut = \"halfZoomOut\"\n",
        "    shortZoomIn = \"shortZoomIn\"\n",
        "    shortZoomOut = \"shortZoomOut\"\n",
        "    shortArcShotLeft = \"shortArcShotLeft\"\n",
        "    shortArcShotRight = \"shortArcShotRight\"\n",
        "    halfArcShotLeft = \"halfArcShotLeft\"\n",
        "    halfArcShotRight = \"halfArcShotRight\"\n",
        "    fullArcShotLeft = \"fullArcShotLeft\"\n",
        "    fullArcShotRight = \"fullArcShotRight\"\n",
        "    panAndTilt = \"panAndTilt\"\n",
        "    dollyAndPan = \"dollyAndPan\"\n",
        "    zoomAndTruck = \"zoomAndTruck\"\n",
        "\n",
        "class EasingType(Enum):\n",
        "    linear = \"linear\"\n",
        "    easeInQuad = \"easeInQuad\"\n",
        "    easeInCubic = \"easeInCubic\"\n",
        "    easeInQuart = \"easeInQuart\"\n",
        "    easeInQuint = \"easeInQuint\"\n",
        "    easeOutQuad = \"easeOutQuad\"\n",
        "    easeOutCubic = \"easeOutCubic\"\n",
        "    easeOutQuart = \"easeOutQuart\"\n",
        "    easeOutQuint = \"easeOutQuint\"\n",
        "    easeInOutQuad = \"easeInOutQuad\"\n",
        "    easeInOutCubic = \"easeInOutCubic\"\n",
        "    easeInOutQuart = \"easeInOutQuart\"\n",
        "    easeInOutQuint = \"easeInOutQuint\"\n",
        "    easeInSine = \"easeInSine\"\n",
        "    easeOutSine = \"easeOutSine\"\n",
        "    easeInOutSine = \"easeInOutSine\"\n",
        "    easeInExpo = \"easeInExpo\"\n",
        "    easeOutExpo = \"easeOutExpo\"\n",
        "    easeInOutExpo = \"easeInOutExpo\"\n",
        "    easeInCirc = \"easeInCirc\"\n",
        "    easeOutCirc = \"easeOutCirc\"\n",
        "    easeInOutCirc = \"easeInOutCirc\"\n",
        "    easeInBounce = \"easeInBounce\"\n",
        "    easeOutBounce = \"easeOutBounce\"\n",
        "    easeInOutBounce = \"easeInOutBounce\"\n",
        "    easeInElastic = \"easeInElastic\"\n",
        "    easeOutElastic = \"easeOutElastic\"\n",
        "    easeInOutElastic = \"easeInOutElastic\"\n",
        "\n",
        "class CameraAngle(Enum):\n",
        "    lowAngle = \"lowAngle\"\n",
        "    mediumAngle = \"mediumAngle\"\n",
        "    highAngle = \"highAngle\"\n",
        "    birdsEyeView = \"birdsEyeView\"\n",
        "\n",
        "class ShotType(Enum):\n",
        "    closeUp = \"closeUp\"\n",
        "    mediumShot = \"mediumShot\"\n",
        "    longShot = \"longShot\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xUiOirJXjD_5"
      },
      "outputs": [],
      "source": [
        "def get_movement_description(movement, easing, camera_angle=None, shot_type=None):\n",
        "    movement_descriptions = {\n",
        "        \"panLeft\": \"panning left\",\n",
        "        \"panRight\": \"panning right\",\n",
        "        \"tiltUp\": \"tilting up\",\n",
        "        \"tiltDown\": \"tilting down\",\n",
        "        \"dollyIn\": \"moving closer\",\n",
        "        \"dollyOut\": \"moving away\",\n",
        "        \"truckLeft\": \"moving left\",\n",
        "        \"truckRight\": \"moving right\",\n",
        "        \"pedestalUp\": \"rising vertically\",\n",
        "        \"pedestalDown\": \"descending vertically\",\n",
        "        \"fullZoomIn\": \"zooming in fully\",\n",
        "        \"fullZoomOut\": \"zooming out fully\",\n",
        "        \"halfZoomIn\": \"zooming in halfway\",\n",
        "        \"halfZoomOut\": \"zooming out halfway\",\n",
        "        \"shortZoomIn\": \"zooming in slightly\",\n",
        "        \"shortZoomOut\": \"zooming out slightly\",\n",
        "        \"shortArcShotLeft\": \"moving in a short arc to the left\",\n",
        "        \"shortArcShotRight\": \"moving in a short arc to the right\",\n",
        "        \"halfArcShotLeft\": \"moving in a half arc to the left\",\n",
        "        \"halfArcShotRight\": \"moving in a half arc to the right\",\n",
        "        \"fullArcShotLeft\": \"moving in a full arc to the left\",\n",
        "        \"fullArcShotRight\": \"moving in a full arc to the right\",\n",
        "        \"panAndTilt\": \"panning and tilting\",\n",
        "        \"dollyAndPan\": \"moving and panning\",\n",
        "        \"zoomAndTruck\": \"zooming and moving sideways\",\n",
        "    }\n",
        "\n",
        "    easing_descriptions = {\n",
        "        \"linear\": \"at a constant speed\",\n",
        "        \"easeInQuad\": \"slowly at first, then accelerating gradually\",\n",
        "        \"easeInCubic\": \"slowly at first, then accelerating more rapidly\",\n",
        "        \"easeInQuart\": \"very slowly at first, then accelerating dramatically\",\n",
        "        \"easeInQuint\": \"extremely slowly at first, then accelerating very dramatically\",\n",
        "        \"easeOutQuad\": \"quickly at first, then decelerating gradually\",\n",
        "        \"easeOutCubic\": \"quickly at first, then decelerating more rapidly\",\n",
        "        \"easeOutQuart\": \"very quickly at first, then decelerating dramatically\",\n",
        "        \"easeOutQuint\": \"extremely quickly at first, then decelerating very dramatically\",\n",
        "        \"easeInOutQuad\": \"gradually accelerating, then gradually decelerating\",\n",
        "        \"easeInOutCubic\": \"slowly accelerating, then decelerating more rapidly\",\n",
        "        \"easeInOutQuart\": \"slowly accelerating, then decelerating dramatically\",\n",
        "        \"easeInOutQuint\": \"very slowly accelerating, then decelerating very dramatically\",\n",
        "        \"easeInSine\": \"with a gentle start, gradually increasing in speed\",\n",
        "        \"easeOutSine\": \"quickly at first, then gently decelerating\",\n",
        "        \"easeInOutSine\": \"with a gentle start and end, faster in the middle\",\n",
        "        \"easeInExpo\": \"starting very slowly, then accelerating exponentially\",\n",
        "        \"easeOutExpo\": \"starting very fast, then decelerating exponentially\",\n",
        "        \"easeInOutExpo\": \"starting and ending slowly, with rapid acceleration and deceleration in the middle\",\n",
        "        \"easeInCirc\": \"starting slowly, then accelerating sharply towards the end\",\n",
        "        \"easeOutCirc\": \"starting quickly, then decelerating sharply towards the end\",\n",
        "        \"easeInOutCirc\": \"with sharp acceleration and deceleration at both ends\",\n",
        "        \"easeInBounce\": \"with a bouncing effect that intensifies towards the end\",\n",
        "        \"easeOutBounce\": \"quickly at first, then bouncing to a stop\",\n",
        "        \"easeInOutBounce\": \"with a bouncing effect at both the start and end\",\n",
        "        \"easeInElastic\": \"with an elastic effect that intensifies towards the end\",\n",
        "        \"easeOutElastic\": \"quickly at first, then oscillating to a stop\",\n",
        "        \"easeInOutElastic\": \"with an elastic effect at both the start and end\",\n",
        "    }\n",
        "\n",
        "    angle_descriptions = {\n",
        "        \"lowAngle\": \"from a low angle\",\n",
        "        \"mediumAngle\": \"from a medium angle\",\n",
        "        \"highAngle\": \"from a high angle\",\n",
        "        \"birdsEyeView\": \"from a bird's eye view\",\n",
        "    }\n",
        "\n",
        "    shot_descriptions = {\n",
        "        \"closeUp\": \"in a close-up shot\",\n",
        "        \"mediumShot\": \"in a medium shot\",\n",
        "        \"longShot\": \"in a long shot\",\n",
        "    }\n",
        "\n",
        "    description = f\"The camera is {movement_descriptions.get(movement, movement)}\"\n",
        "\n",
        "    if easing in easing_descriptions:\n",
        "        description += f\" {easing_descriptions[easing]}\"\n",
        "    else:\n",
        "        description += f\" with {easing} easing\"\n",
        "\n",
        "    if camera_angle:\n",
        "        description += f\", {angle_descriptions.get(camera_angle, camera_angle)}\"\n",
        "\n",
        "    if shot_type:\n",
        "        description += f\" {shot_descriptions.get(shot_type, shot_type)}\"\n",
        "\n",
        "    return description"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dT895usiy-v9"
      },
      "outputs": [],
      "source": [
        "def get_clip_embedding(text: str) -> torch.Tensor:\n",
        "    inputs = clip_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "    with torch.no_grad():\n",
        "        embedding = clip_text_encoder(**inputs).pooler_output\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "m-Ag9MugZJwF"
      },
      "outputs": [],
      "source": [
        "def get_all_movement_descriptions():\n",
        "    descriptions = []\n",
        "    for movement in CameraMovementType:\n",
        "        for easing in EasingType:\n",
        "            descriptions.append(get_movement_description(movement.value, easing.value))\n",
        "            for shot in ShotType:\n",
        "                descriptions.append(get_movement_description(movement.value, easing.value, shot_type=shot.value))\n",
        "            for angle in CameraAngle:\n",
        "                descriptions.append(get_movement_description(movement.value, easing.value, camera_angle=angle.value))\n",
        "                for shot in ShotType:\n",
        "                    descriptions.append(get_movement_description(movement.value, easing.value, camera_angle=angle.value, shot_type=shot.value))\n",
        "    return descriptions\n",
        "\n",
        "def get_movement_index(movement, easing, camera_angle=None, shot_type=None):\n",
        "    description = get_movement_description(movement, easing, camera_angle, shot_type)\n",
        "    return all_movement_descriptions.index(description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "u621kky1Zm8u"
      },
      "outputs": [],
      "source": [
        "def autoregressive_decode(decoder, latent, num_frames, initial_input):\n",
        "    current_input = initial_input.to(device)\n",
        "    trajectory = []\n",
        "\n",
        "    for _ in range(num_frames):\n",
        "        output = decoder(latent, current_input)\n",
        "        next_step = output[:, -1:, :]\n",
        "        trajectory.append(next_step)\n",
        "        current_input = torch.cat([current_input, next_step], dim=1)\n",
        "\n",
        "    return torch.cat(trajectory, dim=1)\n",
        "\n",
        "def generate_camera_movement(model: torch.nn.Module, text_input: str, num_frames: int = 30) -> List[Dict]:\n",
        "    with torch.no_grad():\n",
        "        latent = get_clip_embedding(text_input).to(device)\n",
        "        initial_input = torch.zeros(1, 1, 7).to(device)\n",
        "\n",
        "        camera_trajectory = autoregressive_decode(model.decoder, latent, num_frames, initial_input)\n",
        "\n",
        "    return process_camera_trajectory(camera_trajectory)\n",
        "\n",
        "def reconstruct_camera_movement(decoder: torch.nn.Module, camera_trajectory: torch.Tensor, mask: torch.Tensor = None, num_frames: int = 30) -> List[Dict]:\n",
        "    camera_trajectory = camera_trajectory.unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        latent = model.encoder(camera_trajectory, mask)\n",
        "        initial_input = torch.zeros(1, 1, 7).to(device)\n",
        "        reconstructed = autoregressive_decode(decoder, latent, num_frames, initial_input)\n",
        "\n",
        "    return process_camera_trajectory(reconstructed)\n",
        "\n",
        "def process_camera_trajectory(trajectory: torch.Tensor) -> List[Dict]:\n",
        "    trajectory = trajectory.cpu().numpy()[0]\n",
        "    camera_frames = []\n",
        "    for frame in trajectory:\n",
        "        position, angle, focal_length = frame[:3], frame[3:6], frame[6]\n",
        "        camera_frame = {\n",
        "            \"position\": {axis: float(value) for axis, value in zip(['x', 'y', 'z'], position)},\n",
        "            \"angle\": {axis: float(value) for axis, value in zip(['x', 'y', 'z'], angle)},\n",
        "            \"focalLength\": float(focal_length)\n",
        "        }\n",
        "        camera_frames.append(camera_frame)\n",
        "    return camera_frames\n",
        "\n",
        "def save_to_json(camera_frames: List[Dict], filename: str):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(camera_frames, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsJo1-aVZaSK"
      },
      "source": [
        "#Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hU3OzBiKt2dt"
      },
      "outputs": [],
      "source": [
        "clip_model_name = \"openai/clip-vit-large-patch14\" #@param [\"openai/clip-vit-base-patch32\", \"openai/clip-vit-large-patch14\"]\n",
        "\n",
        "if clip_model_name == \"openai/clip-vit-large-patch14\":\n",
        "    clip_embedding_dim = 768\n",
        "else:\n",
        "    clip_embedding_dim = 512\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
        "clip_text_encoder = CLIPTextModel.from_pretrained(clip_model_name).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGc3_s9r1h9s"
      },
      "outputs": [],
      "source": [
        "all_movement_descriptions = get_all_movement_descriptions()\n",
        "\n",
        "batch_size = 100\n",
        "all_clip_text_features = []\n",
        "\n",
        "for i in tqdm(range(0, len(all_movement_descriptions), batch_size), desc=\"Processing batches\"):\n",
        "    batch = all_movement_descriptions[i:i+batch_size]\n",
        "    batch_features = get_clip_embedding(batch)\n",
        "    all_clip_text_features.append(batch_features)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "all_clip_text_features = torch.cat(all_clip_text_features, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "yArArZxWxMMj"
      },
      "outputs": [],
      "source": [
        "class SimulationDataset(Dataset):\n",
        "    def __init__(self, json_file_path: str):\n",
        "        with open(json_file_path, 'r') as file:\n",
        "            raw_data = json.load(file)\n",
        "        self.simulation_data = [self._process_single_simulation(sim) for sim in raw_data['simulations']\n",
        "                if self._is_simulation_valid(sim)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.simulation_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        simulation = self.simulation_data[index]\n",
        "        return {\n",
        "            'camera_trajectory': torch.tensor(simulation['camera_trajectory'], dtype=torch.float32),\n",
        "            'movement_type': torch.tensor(simulation['movement_type'], dtype=torch.long),\n",
        "            'easing_type': torch.tensor(simulation['easing_type'], dtype=torch.long),\n",
        "            'label_index': simulation['label_index']\n",
        "        }\n",
        "\n",
        "    def _is_simulation_valid(self, simulation):\n",
        "        return (len(simulation['instructions']) == 1 and\n",
        "                simulation['instructions'][0]['frameCount'] == 30 and\n",
        "                len(simulation['cameraFrames']) == 30)\n",
        "\n",
        "    def _process_single_simulation(self, simulation):\n",
        "        instruction = simulation['instructions'][0]\n",
        "        subject = simulation['subjects'][0]\n",
        "        subject_center = np.array([subject['position']['x'], subject['position']['y'], subject['position']['z']])\n",
        "        subject_size = np.array([subject['size']['x'], subject['size']['y'], subject['size']['z']])\n",
        "        subject_area = 1\n",
        "\n",
        "        camera_trajectory = self._normalize_camera_trajectory(simulation['cameraFrames'], subject_center, subject_area)\n",
        "\n",
        "        movement_type = CameraMovementType[instruction['cameraMovement']].value\n",
        "        easing_type = EasingType[instruction['movementEasing']].value\n",
        "        camera_angle = CameraAngle[instruction.get('initialCameraAngle')].value if 'initialCameraAngle' in instruction else None\n",
        "        shot_type = ShotType[instruction.get('initialShotType')].value if 'initialShotType' in instruction else None\n",
        "\n",
        "        movement_type_index = list(CameraMovementType).index(CameraMovementType(movement_type))\n",
        "        easing_type_index = list(EasingType).index(EasingType(easing_type))\n",
        "\n",
        "        label_index = get_movement_index(movement_type, easing_type, camera_angle, shot_type)\n",
        "\n",
        "        return {\n",
        "            'camera_trajectory': camera_trajectory,\n",
        "            'movement_type': movement_type_index,\n",
        "            'easing_type': easing_type_index,\n",
        "            'label_index': label_index\n",
        "        }\n",
        "\n",
        "    def _normalize_camera_trajectory(self, camera_frames, subject_center, subject_area):\n",
        "        trajectory = []\n",
        "        for frame in camera_frames:\n",
        "            relative_position = (np.array([frame['position']['x'], frame['position']['y'], frame['position']['z']]) - subject_center) * subject_area\n",
        "            trajectory.extend(relative_position.tolist())\n",
        "            trajectory.extend([frame['angle']['x'], frame['angle']['y'], frame['angle']['z']])\n",
        "            trajectory.append(frame['focalLength'])\n",
        "        return trajectory\n",
        "\n",
        "def batch_collate(batch):\n",
        "    return {\n",
        "        'camera_trajectory': torch.stack([item['camera_trajectory'] for item in batch]),\n",
        "        'movement_type': torch.stack([item['movement_type'] for item in batch]),\n",
        "        'easing_type': torch.stack([item['easing_type'] for item in batch]),\n",
        "        'positive_indices': torch.tensor([item['label_index'] for item in batch], dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pu60D7QlzDJR"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module): # Sinusoidal Positional Encoding\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, max_seq_length, latent_dim):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        self.input_projection = nn.Linear(7, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        self.encoder_to_latent = nn.Linear(d_model * max_seq_length, latent_dim)\n",
        "\n",
        "    def forward(self, src, mask=None):\n",
        "        batch_size = src.size(0)\n",
        "        src = src.view(batch_size, self.max_seq_length, 7)\n",
        "        src = self.input_projection(src)\n",
        "        src = src.permute(1, 0, 2)\n",
        "        src = self.pos_encoder(src)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask[:, :self.max_seq_length]\n",
        "            mask = mask.permute(1, 0).unsqueeze(-1).expand(-1, -1, self.d_model)\n",
        "            src = src * mask\n",
        "            src_key_padding_mask = ~mask[:, :, 0].permute(1, 0).bool()\n",
        "        else:\n",
        "            src_key_padding_mask = None\n",
        "\n",
        "        embedding = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)\n",
        "        latent = self.encoder_to_latent(embedding.permute(1, 0, 2).reshape(batch_size, -1))\n",
        "        return latent\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_decoder_layers, max_seq_length, latent_dim):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        self.latent_to_memory = nn.Linear(latent_dim, d_model * max_seq_length)\n",
        "        self.input_projection = nn.Linear(7, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.output_projection = nn.Linear(d_model, 7)\n",
        "\n",
        "    def forward(self, latent, tgt):\n",
        "        memory = self.latent_to_memory(latent).view(-1, self.max_seq_length, self.d_model).permute(1, 0, 2)\n",
        "        tgt = self.input_projection(tgt)\n",
        "        tgt = tgt.permute(1, 0, 2)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "        output = self.transformer_decoder(tgt, memory)\n",
        "        output = output.permute(1, 0, 2)\n",
        "        output = self.output_projection(output)\n",
        "        return output\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, latent_dim, num_movement_types, num_easing_types):\n",
        "        super(Classifier, self).__init__()\n",
        "        self.movement_type_classifier = nn.Linear(latent_dim, num_movement_types)\n",
        "        self.easing_type_classifier = nn.Linear(latent_dim, num_easing_types)\n",
        "\n",
        "    def forward(self, latent):\n",
        "        movement_type_logits = self.movement_type_classifier(latent)\n",
        "        easing_type_logits = self.easing_type_classifier(latent)\n",
        "        return movement_type_logits, easing_type_logits\n",
        "\n",
        "class MultiTaskAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
        "                 num_movement_types, num_easing_types, max_seq_length, latent_dim):\n",
        "        super(MultiTaskAutoencoder, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, d_model, nhead, num_encoder_layers, max_seq_length, latent_dim)\n",
        "        self.decoder = Decoder(d_model, nhead, num_decoder_layers, max_seq_length, latent_dim)\n",
        "        self.classifier = Classifier(latent_dim, num_movement_types, num_easing_types)\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "    def forward(self, input, mask=None):\n",
        "        latent = self.encoder(input, mask)\n",
        "\n",
        "        movement_type_logits, easing_type_logits = self.classifier(latent)\n",
        "\n",
        "        initial_input = torch.zeros(input.shape[0], 1, 7).to(device)\n",
        "        reconstructed = autoregressive_decode(self.decoder, latent, self.max_seq_length, initial_input)\n",
        "\n",
        "        return {\n",
        "            'latent': latent,\n",
        "            'movement_type_logits': movement_type_logits,\n",
        "            'easing_type_logits': easing_type_logits,\n",
        "            'reconstructed': reconstructed.view(-1, self.max_seq_length * 7),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zpbK0hL7rwH_"
      },
      "outputs": [],
      "source": [
        "def create_mask(batch_size, seq_length, mask_ratio=0.5):\n",
        "    return torch.bernoulli(torch.full((batch_size, seq_length), 1 - mask_ratio)).bool().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5_OietBldogG"
      },
      "outputs": [],
      "source": [
        "def contrastive_loss(latent, clip_text_features, positive_indices, temperature=0.5, num_samples=50):\n",
        "    latent = F.normalize(latent, p=2, dim=1)\n",
        "    clip_text_features = F.normalize(clip_text_features, p=2, dim=1)\n",
        "\n",
        "    num_features = clip_text_features.size(0)\n",
        "    random_indices = torch.randperm(num_features, device=device)[:num_samples]\n",
        "    sampled_features = clip_text_features[random_indices]\n",
        "\n",
        "    similarity_matrix = torch.matmul(latent, sampled_features.T) / temperature\n",
        "\n",
        "    positive_mask = torch.zeros(similarity_matrix.shape, dtype=torch.bool, device=device)\n",
        "    for i, idx in enumerate(positive_indices):\n",
        "        if (random_indices == idx).any():\n",
        "            positive_mask[i, torch.where(random_indices == idx)[0]] = True\n",
        "\n",
        "    if positive_mask.any():\n",
        "        target = torch.zeros(similarity_matrix.shape[0], dtype=torch.long, device=device)\n",
        "        for i in range(similarity_matrix.shape[0]):\n",
        "            positive_indices = torch.where(positive_mask[i])[0]\n",
        "            if len(positive_indices) > 0:\n",
        "                target[i] = positive_indices[0]\n",
        "            else:\n",
        "                target[i] = 0\n",
        "\n",
        "        loss = F.cross_entropy(similarity_matrix, target, reduction='none')\n",
        "        valid_loss = loss[positive_mask.any(dim=1)]\n",
        "        return valid_loss.mean() if len(valid_loss) > 0 else torch.tensor(0.0, device=device, requires_grad=True)\n",
        "    else:\n",
        "        return torch.tensor(0.0, device=device, requires_grad=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "W2OsT98BzXKy"
      },
      "outputs": [],
      "source": [
        "def init_losses():\n",
        "  return {\n",
        "      'total': 0,\n",
        "      'movement_type': 0,\n",
        "      'easing_type': 0,\n",
        "      'reconstruction': 0,\n",
        "      'clip': 0,\n",
        "      'clip_contrastive': 0\n",
        "  }\n",
        "\n",
        "def print_detailed_losses(phase, losses):\n",
        "    print(f\"{phase} Losses - \"\n",
        "          f\"Movement: {losses['movement_type']:.4f}, \"\n",
        "          f\"Easing: {losses['easing_type']:.4f}, \"\n",
        "          f\"Reconstruction: {losses['reconstruction']:.4f}, \"\n",
        "          f\"CLIP: {losses['clip']:.4f}, \"\n",
        "          f\"CLIP Contrastive: {losses['clip_contrastive']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eglTw5WODOIO"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion, all_clip_text_features):\n",
        "    model.train()\n",
        "    total_losses = init_losses()\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        losses = process_batch(model, batch, criterion, all_clip_text_features)\n",
        "\n",
        "        loss = sum(losses.values())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for key in total_losses:\n",
        "            total_losses[key] += losses[key].item() if torch.is_tensor(losses[key]) else losses[key]\n",
        "\n",
        "    return {k: v / len(dataloader) for k, v in total_losses.items()}\n",
        "\n",
        "def validate(model, dataloader, criterion, all_clip_text_features):\n",
        "    model.eval()\n",
        "    total_losses = init_losses()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            losses = process_batch(model, batch, criterion, all_clip_text_features)\n",
        "            loss = sum(losses.values())\n",
        "\n",
        "            for key in total_losses:\n",
        "                total_losses[key] += losses[key].item() if torch.is_tensor(losses[key]) else losses[key]\n",
        "\n",
        "    return {k: v / len(dataloader) for k, v in total_losses.items()}\n",
        "\n",
        "def process_batch(model, batch, criterion, all_clip_text_features):\n",
        "    camera_trajectory = batch['camera_trajectory'].to(device)\n",
        "    movement_type = batch['movement_type'].to(device)\n",
        "    easing_type = batch['easing_type'].to(device)\n",
        "    positive_indices = batch['positive_indices'].to(device)\n",
        "\n",
        "    mask = create_mask(camera_trajectory.shape[0], camera_trajectory.shape[1])\n",
        "    output = model(camera_trajectory, mask)\n",
        "\n",
        "    losses = {\n",
        "        'movement_type': criterion['classification'](output['movement_type_logits'], movement_type),\n",
        "        'easing_type': criterion['classification'](output['easing_type_logits'], easing_type),\n",
        "        'reconstruction': criterion['reconstruction'](output['reconstructed'], camera_trajectory),\n",
        "        'clip': 1 - F.cosine_similarity(output['latent'], all_clip_text_features[positive_indices]).mean(),\n",
        "        'clip_contrastive': contrastive_loss(output['latent'], all_clip_text_features, positive_indices)\n",
        "    }\n",
        "    losses['total'] = sum(losses.values())\n",
        "\n",
        "    return losses\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, config):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "    criterion = {\n",
        "        'classification': nn.CrossEntropyLoss(),\n",
        "        'reconstruction': nn.MSELoss()\n",
        "    }\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        train_losses = train_epoch(model, train_dataloader, optimizer, criterion, config['all_clip_text_features'])\n",
        "        val_losses = validate(model, val_dataloader, criterion, config['all_clip_text_features'])\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
        "        print(f\"Train Loss: {train_losses['total']:.4f}, Validation Loss: {val_losses['total']:.4f}\")\n",
        "        print_detailed_losses(\"Train\", train_losses)\n",
        "        print_detailed_losses(\"Valid\", val_losses)\n",
        "\n",
        "        if val_losses['total'] < best_val_loss:\n",
        "            best_val_loss = val_losses['total']\n",
        "            epochs_without_improvement = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(\"New best model saved!\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        if epochs_without_improvement >= config['patience']:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GPK8Tp53m8d"
      },
      "outputs": [],
      "source": [
        "dataset = SimulationDataset('random_simulation_dataset.json')\n",
        "\n",
        "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=batch_collate)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=batch_collate)\n",
        "\n",
        "input_dim = 30 * 7  # 30 camera_trajectory, 7 values per frame\n",
        "d_model = 256\n",
        "nhead = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "num_movement_types = len(CameraMovementType)\n",
        "num_easing_types = len(EasingType)\n",
        "max_seq_length = 30\n",
        "latent_dim = clip_embedding_dim\n",
        "\n",
        "model = MultiTaskAutoencoder(input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
        "                             num_movement_types, num_easing_types, max_seq_length, latent_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8zCiaVe3ltM"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'num_epochs': 100,\n",
        "    'patience': 10,\n",
        "    'learning_rate': 0.001,\n",
        "    'all_clip_text_features': all_clip_text_features\n",
        "}\n",
        "\n",
        "train_model(model, train_dataloader, val_dataloader, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONMw8qWQkKqN"
      },
      "source": [
        "##Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp5tXDnz02Uj"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Il2pEix4xQEv"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/result.json'\n",
        "\n",
        "input_text = \"The camera is moving in a full arc to the left with a bouncing effect that intensifies towards the end in a close-up shot\" # The camera is panning left at a constant speed, from a low angle in a close-up shot\n",
        "camera_frames = generate_camera_movement(model, input_text)\n",
        "save_to_json(camera_frames, file_path)\n",
        "\n",
        "files.download(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEKo1EUbn25b"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/result.json'\n",
        "\n",
        "camera_frames = reconstruct_camera_movement(model.decoder, dataset[1]['camera_trajectory'].to(device))\n",
        "save_to_json(camera_frames, file_path)\n",
        "\n",
        "files.download(file_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
