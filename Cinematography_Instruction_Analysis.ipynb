{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZahraDehghanian97/LenseCraft/blob/master/Cinematography_Instruction_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "from enum import Enum\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from transformers import CLIPTextModel, CLIPTokenizer"
      ],
      "metadata": {
        "id": "uqHtkNVtxah8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1VT2XfBj9LFWLUBjv65dzC4bVzH0zdNDU"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpUEXi1jxsMW",
        "outputId": "f9046cdc-e21f-424c-cc20-bb9088552f33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VT2XfBj9LFWLUBjv65dzC4bVzH0zdNDU\n",
            "To: /content/random_simulation_dataset.json\n",
            "\r  0% 0.00/1.06M [00:00<?, ?B/s]\r100% 1.06M/1.06M [00:00<00:00, 136MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CameraMovement(Enum):\n",
        "    panLeft = \"panLeft\"\n",
        "    panRight = \"panRight\"\n",
        "    tiltUp = \"tiltUp\"\n",
        "    tiltDown = \"tiltDown\"\n",
        "    dollyIn = \"dollyIn\"\n",
        "    dollyOut = \"dollyOut\"\n",
        "    truckLeft = \"truckLeft\"\n",
        "    truckRight = \"truckRight\"\n",
        "    pedestalUp = \"pedestalUp\"\n",
        "    pedestalDown = \"pedestalDown\"\n",
        "    fullZoomIn = \"fullZoomIn\"\n",
        "    fullZoomOut = \"fullZoomOut\"\n",
        "    halfZoomIn = \"halfZoomIn\"\n",
        "    halfZoomOut = \"halfZoomOut\"\n",
        "    shortZoomIn = \"shortZoomIn\"\n",
        "    shortZoomOut = \"shortZoomOut\"\n",
        "    shortArcShotLeft = \"shortArcShotLeft\"\n",
        "    shortArcShotRight = \"shortArcShotRight\"\n",
        "    halfArcShotLeft = \"halfArcShotLeft\"\n",
        "    halfArcShotRight = \"halfArcShotRight\"\n",
        "    fullArcShotLeft = \"fullArcShotLeft\"\n",
        "    fullArcShotRight = \"fullArcShotRight\"\n",
        "    panAndTilt = \"panAndTilt\"\n",
        "    dollyAndPan = \"dollyAndPan\"\n",
        "    zoomAndTruck = \"zoomAndTruck\"\n",
        "\n",
        "class MovementEasing(Enum):\n",
        "    linear = \"linear\"\n",
        "    easeInQuad = \"easeInQuad\"\n",
        "    easeInCubic = \"easeInCubic\"\n",
        "    easeInQuart = \"easeInQuart\"\n",
        "    easeInQuint = \"easeInQuint\"\n",
        "    easeOutQuad = \"easeOutQuad\"\n",
        "    easeOutCubic = \"easeOutCubic\"\n",
        "    easeOutQuart = \"easeOutQuart\"\n",
        "    easeOutQuint = \"easeOutQuint\"\n",
        "    easeInOutQuad = \"easeInOutQuad\"\n",
        "    easeInOutCubic = \"easeInOutCubic\"\n",
        "    easeInOutQuart = \"easeInOutQuart\"\n",
        "    easeInOutQuint = \"easeInOutQuint\"\n",
        "    easeInSine = \"easeInSine\"\n",
        "    easeOutSine = \"easeOutSine\"\n",
        "    easeInOutSine = \"easeInOutSine\"\n",
        "    easeInExpo = \"easeInExpo\"\n",
        "    easeOutExpo = \"easeOutExpo\"\n",
        "    easeInOutExpo = \"easeInOutExpo\"\n",
        "    easeInCirc = \"easeInCirc\"\n",
        "    easeOutCirc = \"easeOutCirc\"\n",
        "    easeInOutCirc = \"easeInOutCirc\"\n",
        "    easeInBounce = \"easeInBounce\"\n",
        "    easeOutBounce = \"easeOutBounce\"\n",
        "    easeInOutBounce = \"easeInOutBounce\"\n",
        "    easeInElastic = \"easeInElastic\"\n",
        "    easeOutElastic = \"easeOutElastic\"\n",
        "    easeInOutElastic = \"easeInOutElastic\""
      ],
      "metadata": {
        "id": "gR_z1bzx1YRk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yArArZxWxMMj"
      },
      "outputs": [],
      "source": [
        "class SimulationDataset(Dataset):\n",
        "    def __init__(self, file_path: str):\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        self.processed_data = []\n",
        "\n",
        "        for simulation in data['simulations']:\n",
        "            if len(simulation['instructions']) != 1:\n",
        "                continue\n",
        "\n",
        "            instruction = simulation['instructions'][0]\n",
        "\n",
        "            if instruction['frameCount'] != 30:\n",
        "                continue\n",
        "\n",
        "            camera_frames = simulation['cameraFrames']\n",
        "            if len(camera_frames) != 30:\n",
        "                continue\n",
        "\n",
        "            flattened_frames = []\n",
        "            for frame in camera_frames:\n",
        "                flattened_frames.extend([frame['position']['x'], frame['position']['y'], frame['position']['z']])\n",
        "                flattened_frames.extend([frame['angle']['x'], frame['angle']['y'], frame['angle']['z']])\n",
        "                flattened_frames.append(frame['focalLength'])\n",
        "\n",
        "            camera_movement = CameraMovement[instruction['cameraMovement']].value\n",
        "            movement_easing = MovementEasing[instruction['movementEasing']].value\n",
        "\n",
        "            camera_movement_index = list(CameraMovement).index(CameraMovement(camera_movement))\n",
        "            movement_easing_index = list(MovementEasing).index(MovementEasing(movement_easing))\n",
        "\n",
        "            self.processed_data.append({\n",
        "                'frames': flattened_frames,\n",
        "                'camera_movement': camera_movement_index,\n",
        "                'movement_easing': movement_easing_index,\n",
        "                'instruction_text': f\"{instruction['cameraMovement']} with {instruction['movementEasing']}\"\n",
        "            })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.processed_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.processed_data[idx]\n",
        "        return {\n",
        "            'frames': torch.tensor(item['frames'], dtype=torch.float32),\n",
        "            'camera_movement': torch.tensor(item['camera_movement'], dtype=torch.long),\n",
        "            'movement_easing': torch.tensor(item['movement_easing'], dtype=torch.long),\n",
        "            'instruction_text': item['instruction_text']\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return {\n",
        "        'frames': torch.stack([item['frames'] for item in batch]),\n",
        "        'camera_movement': torch.stack([item['camera_movement'] for item in batch]),\n",
        "        'movement_easing': torch.stack([item['movement_easing'] for item in batch]),\n",
        "        'instruction_text': [item['instruction_text'] for item in batch]\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sinusoidal Positional Encoding\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:x.size(0), :]"
      ],
      "metadata": {
        "id": "aDsKywLQ7SLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiTaskAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
        "                 num_camera_movements, num_movement_easings, max_seq_length, latent_dim):\n",
        "        super(MultiTaskAutoencoder, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.latent_dim = latent_dim\n",
        "\n",
        "        self.input_projection = nn.Linear(7, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        self.latent_projection = nn.Linear(d_model * max_seq_length, latent_dim)\n",
        "\n",
        "        self.camera_movement_classifier = nn.Linear(latent_dim, num_camera_movements)\n",
        "        self.movement_easing_classifier = nn.Linear(latent_dim, num_movement_easings)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.output_projection = nn.Linear(d_model, 7)\n",
        "\n",
        "    def encode(self, src):\n",
        "        src = src.view(-1, self.max_seq_length, 7)\n",
        "        src = self.input_projection(src)\n",
        "        src = src.permute(1, 0, 2)\n",
        "        src = self.pos_encoder(src)\n",
        "        memory = self.transformer_encoder(src)\n",
        "        return memory\n",
        "\n",
        "    def decode(self, memory, tgt):\n",
        "        tgt = self.input_projection(tgt)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "        output = self.transformer_decoder(tgt, memory)\n",
        "        output = self.output_projection(output)\n",
        "        return output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, self.max_seq_length, 7)\n",
        "\n",
        "        memory = self.encode(x)\n",
        "\n",
        "        latent = memory.permute(1, 0, 2).reshape(-1, self.d_model * self.max_seq_length)\n",
        "        latent = self.latent_projection(latent)\n",
        "\n",
        "        camera_movement_logits = self.camera_movement_classifier(latent)\n",
        "        movement_easing_logits = self.movement_easing_classifier(latent)\n",
        "\n",
        "        reconstructed_frames = []\n",
        "        for i in range(self.max_seq_length):\n",
        "            step_input = x[:, :i+1, :].permute(1, 0, 2)\n",
        "            step_output = self.decode(memory, step_input)\n",
        "            reconstructed_frames.append(step_output[-1])\n",
        "\n",
        "        reconstructed = torch.stack(reconstructed_frames, dim=1)\n",
        "\n",
        "        return {\n",
        "            'latent': latent,\n",
        "            'camera_movement_logits': camera_movement_logits,\n",
        "            'movement_easing_logits': movement_easing_logits,\n",
        "            'reconstructed': reconstructed.view(-1, self.max_seq_length * 7),\n",
        "        }"
      ],
      "metadata": {
        "id": "pu60D7QlzDJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset = SimulationDataset('random_simulation_dataset.json')\n",
        "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "input_dim = 30 * 7  # 30 frames, 7 values per frame\n",
        "d_model = 256\n",
        "nhead = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "num_camera_movements = len(CameraMovement)\n",
        "num_movement_easings = len(MovementEasing)\n",
        "max_seq_length = 30\n",
        "clip_model_name = \"openai/clip-vit-base-patch32\"\n",
        "latent_dim = 512\n",
        "\n",
        "model = MultiTaskAutoencoder(input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers,\n",
        "                             num_camera_movements, num_movement_easings, max_seq_length, latent_dim).to(device)\n",
        "\n",
        "clip_tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
        "clip_text_encoder = CLIPTextModel.from_pretrained(clip_model_name).to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "classification_criterion = nn.CrossEntropyLoss()\n",
        "reconstruction_criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "_GPK8Tp53m8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs=100\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    total_camera_movement_loss = 0\n",
        "    total_movement_easing_loss = 0\n",
        "    total_reconstruction_loss = 0\n",
        "    total_clip_loss = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        frames = batch['frames'].to(device)\n",
        "        camera_movement = batch['camera_movement'].to(device)\n",
        "        movement_easing = batch['movement_easing'].to(device)\n",
        "        instruction_text = batch['instruction_text']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(frames)\n",
        "\n",
        "        camera_movement_loss = classification_criterion(output['camera_movement_logits'], camera_movement)\n",
        "        movement_easing_loss = classification_criterion(output['movement_easing_logits'], movement_easing)\n",
        "        reconstruction_loss = reconstruction_criterion(output['reconstructed'], frames)\n",
        "\n",
        "        clip_input = clip_tokenizer(instruction_text, padding=True, truncation=True, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            clip_text_features = clip_text_encoder(**clip_input).last_hidden_state[:, 0, :]\n",
        "\n",
        "        clip_loss = 1 - F.cosine_similarity(output['latent'], clip_text_features).mean()\n",
        "\n",
        "        loss = camera_movement_loss + movement_easing_loss + reconstruction_loss + clip_loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        total_camera_movement_loss += camera_movement_loss.item()\n",
        "        total_movement_easing_loss += movement_easing_loss.item()\n",
        "        total_reconstruction_loss += reconstruction_loss.item()\n",
        "        total_clip_loss += clip_loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    avg_camera_movement_loss = total_camera_movement_loss / len(dataloader)\n",
        "    avg_movement_easing_loss = total_movement_easing_loss / len(dataloader)\n",
        "    avg_reconstruction_loss = total_reconstruction_loss / len(dataloader)\n",
        "    avg_clip_loss = total_clip_loss / len(dataloader)\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}]\")\n",
        "    print(f\"Total Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Camera Movement Loss: {avg_camera_movement_loss:.4f}\")\n",
        "    print(f\"Movement Easing Loss: {avg_movement_easing_loss:.4f}\")\n",
        "    print(f\"Reconstruction Loss: {avg_reconstruction_loss:.4f}\")\n",
        "    print(f\"CLIP Loss: {avg_clip_loss:.4f}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eglTw5WODOIO",
        "outputId": "ddbaf879-78ab-436f-88e4-fc594078c626"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100]\n",
            "Total Loss: 341.3379\n",
            "Camera Movement Loss: 6.5060\n",
            "Movement Easing Loss: 8.3289\n",
            "Reconstruction Loss: 325.5145\n",
            "CLIP Loss: 0.9884\n",
            "--------------------------------------------------\n",
            "Epoch [2/100]\n",
            "Total Loss: 310.5421\n",
            "Camera Movement Loss: 15.0435\n",
            "Movement Easing Loss: 16.9723\n",
            "Reconstruction Loss: 277.5394\n",
            "CLIP Loss: 0.9869\n",
            "--------------------------------------------------\n",
            "Epoch [3/100]\n",
            "Total Loss: 332.9131\n",
            "Camera Movement Loss: 14.4351\n",
            "Movement Easing Loss: 13.8457\n",
            "Reconstruction Loss: 303.7138\n",
            "CLIP Loss: 0.9184\n",
            "--------------------------------------------------\n",
            "Epoch [4/100]\n",
            "Total Loss: 285.4020\n",
            "Camera Movement Loss: 11.6725\n",
            "Movement Easing Loss: 8.5087\n",
            "Reconstruction Loss: 264.2918\n",
            "CLIP Loss: 0.9289\n",
            "--------------------------------------------------\n",
            "Epoch [5/100]\n",
            "Total Loss: 260.6937\n",
            "Camera Movement Loss: 10.1781\n",
            "Movement Easing Loss: 9.7378\n",
            "Reconstruction Loss: 239.8354\n",
            "CLIP Loss: 0.9423\n",
            "--------------------------------------------------\n",
            "Epoch [6/100]\n",
            "Total Loss: 244.6559\n",
            "Camera Movement Loss: 7.4485\n",
            "Movement Easing Loss: 7.9427\n",
            "Reconstruction Loss: 228.3872\n",
            "CLIP Loss: 0.8775\n",
            "--------------------------------------------------\n",
            "Epoch [7/100]\n",
            "Total Loss: 297.8631\n",
            "Camera Movement Loss: 10.0016\n",
            "Movement Easing Loss: 7.9967\n",
            "Reconstruction Loss: 279.0925\n",
            "CLIP Loss: 0.7723\n",
            "--------------------------------------------------\n",
            "Epoch [8/100]\n",
            "Total Loss: 208.1874\n",
            "Camera Movement Loss: 6.9390\n",
            "Movement Easing Loss: 5.6158\n",
            "Reconstruction Loss: 194.9364\n",
            "CLIP Loss: 0.6962\n",
            "--------------------------------------------------\n",
            "Epoch [9/100]\n",
            "Total Loss: 228.6471\n",
            "Camera Movement Loss: 7.2676\n",
            "Movement Easing Loss: 5.2899\n",
            "Reconstruction Loss: 215.4530\n",
            "CLIP Loss: 0.6366\n",
            "--------------------------------------------------\n",
            "Epoch [10/100]\n",
            "Total Loss: 192.2212\n",
            "Camera Movement Loss: 6.2988\n",
            "Movement Easing Loss: 5.3959\n",
            "Reconstruction Loss: 179.9473\n",
            "CLIP Loss: 0.5792\n",
            "--------------------------------------------------\n",
            "Epoch [11/100]\n",
            "Total Loss: 199.2991\n",
            "Camera Movement Loss: 5.6691\n",
            "Movement Easing Loss: 4.9600\n",
            "Reconstruction Loss: 188.2290\n",
            "CLIP Loss: 0.4410\n",
            "--------------------------------------------------\n",
            "Epoch [12/100]\n",
            "Total Loss: 165.3892\n",
            "Camera Movement Loss: 5.3459\n",
            "Movement Easing Loss: 4.5044\n",
            "Reconstruction Loss: 155.2387\n",
            "CLIP Loss: 0.3002\n",
            "--------------------------------------------------\n",
            "Epoch [13/100]\n",
            "Total Loss: 159.7148\n",
            "Camera Movement Loss: 4.6725\n",
            "Movement Easing Loss: 4.3661\n",
            "Reconstruction Loss: 150.4425\n",
            "CLIP Loss: 0.2337\n",
            "--------------------------------------------------\n",
            "Epoch [14/100]\n",
            "Total Loss: 182.9366\n",
            "Camera Movement Loss: 4.3359\n",
            "Movement Easing Loss: 4.7754\n",
            "Reconstruction Loss: 173.6259\n",
            "CLIP Loss: 0.1994\n",
            "--------------------------------------------------\n",
            "Epoch [15/100]\n",
            "Total Loss: 136.3861\n",
            "Camera Movement Loss: 3.9363\n",
            "Movement Easing Loss: 3.9857\n",
            "Reconstruction Loss: 128.3212\n",
            "CLIP Loss: 0.1429\n",
            "--------------------------------------------------\n",
            "Epoch [16/100]\n",
            "Total Loss: 150.0241\n",
            "Camera Movement Loss: 5.1902\n",
            "Movement Easing Loss: 4.2209\n",
            "Reconstruction Loss: 140.4634\n",
            "CLIP Loss: 0.1496\n",
            "--------------------------------------------------\n",
            "Epoch [17/100]\n",
            "Total Loss: 125.2600\n",
            "Camera Movement Loss: 4.6401\n",
            "Movement Easing Loss: 4.0101\n",
            "Reconstruction Loss: 116.4553\n",
            "CLIP Loss: 0.1545\n",
            "--------------------------------------------------\n",
            "Epoch [18/100]\n",
            "Total Loss: 136.1085\n",
            "Camera Movement Loss: 4.0787\n",
            "Movement Easing Loss: 3.9572\n",
            "Reconstruction Loss: 127.9366\n",
            "CLIP Loss: 0.1361\n",
            "--------------------------------------------------\n",
            "Epoch [19/100]\n",
            "Total Loss: 128.7508\n",
            "Camera Movement Loss: 4.1441\n",
            "Movement Easing Loss: 3.7052\n",
            "Reconstruction Loss: 120.7697\n",
            "CLIP Loss: 0.1318\n",
            "--------------------------------------------------\n",
            "Epoch [20/100]\n",
            "Total Loss: 94.7540\n",
            "Camera Movement Loss: 3.9641\n",
            "Movement Easing Loss: 4.1044\n",
            "Reconstruction Loss: 86.5622\n",
            "CLIP Loss: 0.1233\n",
            "--------------------------------------------------\n",
            "Epoch [21/100]\n",
            "Total Loss: 86.3518\n",
            "Camera Movement Loss: 3.5677\n",
            "Movement Easing Loss: 3.8598\n",
            "Reconstruction Loss: 78.8301\n",
            "CLIP Loss: 0.0942\n",
            "--------------------------------------------------\n",
            "Epoch [22/100]\n",
            "Total Loss: 83.9002\n",
            "Camera Movement Loss: 3.6926\n",
            "Movement Easing Loss: 3.8320\n",
            "Reconstruction Loss: 76.2931\n",
            "CLIP Loss: 0.0826\n",
            "--------------------------------------------------\n",
            "Epoch [23/100]\n",
            "Total Loss: 79.7289\n",
            "Camera Movement Loss: 3.6881\n",
            "Movement Easing Loss: 3.6730\n",
            "Reconstruction Loss: 72.2798\n",
            "CLIP Loss: 0.0880\n",
            "--------------------------------------------------\n",
            "Epoch [24/100]\n",
            "Total Loss: 69.0213\n",
            "Camera Movement Loss: 3.9461\n",
            "Movement Easing Loss: 4.0034\n",
            "Reconstruction Loss: 60.9748\n",
            "CLIP Loss: 0.0970\n",
            "--------------------------------------------------\n",
            "Epoch [25/100]\n",
            "Total Loss: 75.8928\n",
            "Camera Movement Loss: 3.6338\n",
            "Movement Easing Loss: 3.5695\n",
            "Reconstruction Loss: 68.6165\n",
            "CLIP Loss: 0.0730\n",
            "--------------------------------------------------\n",
            "Epoch [26/100]\n",
            "Total Loss: 59.2735\n",
            "Camera Movement Loss: 3.8141\n",
            "Movement Easing Loss: 3.9401\n",
            "Reconstruction Loss: 51.4272\n",
            "CLIP Loss: 0.0920\n",
            "--------------------------------------------------\n",
            "Epoch [27/100]\n",
            "Total Loss: 67.7498\n",
            "Camera Movement Loss: 3.5892\n",
            "Movement Easing Loss: 3.8568\n",
            "Reconstruction Loss: 60.2133\n",
            "CLIP Loss: 0.0904\n",
            "--------------------------------------------------\n",
            "Epoch [28/100]\n",
            "Total Loss: 50.4622\n",
            "Camera Movement Loss: 3.7727\n",
            "Movement Easing Loss: 3.6753\n",
            "Reconstruction Loss: 42.9335\n",
            "CLIP Loss: 0.0806\n",
            "--------------------------------------------------\n",
            "Epoch [29/100]\n",
            "Total Loss: 46.5448\n",
            "Camera Movement Loss: 3.8573\n",
            "Movement Easing Loss: 3.6449\n",
            "Reconstruction Loss: 38.9766\n",
            "CLIP Loss: 0.0661\n",
            "--------------------------------------------------\n",
            "Epoch [30/100]\n",
            "Total Loss: 58.3563\n",
            "Camera Movement Loss: 3.7937\n",
            "Movement Easing Loss: 3.6416\n",
            "Reconstruction Loss: 50.8571\n",
            "CLIP Loss: 0.0640\n",
            "--------------------------------------------------\n",
            "Epoch [31/100]\n",
            "Total Loss: 40.4414\n",
            "Camera Movement Loss: 3.8962\n",
            "Movement Easing Loss: 3.9422\n",
            "Reconstruction Loss: 32.5213\n",
            "CLIP Loss: 0.0817\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-101-95dad95487f9>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mcamera_movement_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassification_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'camera_movement_logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcamera_movement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-97-9449c043b55b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mstep_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mstep_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m             \u001b[0mreconstructed_frames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-97-9449c043b55b>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, memory, tgt)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mtgt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_decoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_projection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 495\u001b[0;31m             output = mod(output, memory, tgt_mask=tgt_mask,\n\u001b[0m\u001b[1;32m    496\u001b[0m                          \u001b[0mmemory_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m                          \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sa_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mha_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_key_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_is_causal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ff_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py\u001b[0m in \u001b[0;36m_sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    897\u001b[0m     def _sa_block(self, x: Tensor,\n\u001b[1;32m    898\u001b[0m                   attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: bool = False) -> Tensor:\n\u001b[0;32m--> 899\u001b[0;31m         x = self.self_attn(x, x, x,\n\u001b[0m\u001b[1;32m    900\u001b[0m                            \u001b[0mattn_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattn_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m                            \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1553\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1560\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1561\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1273\u001b[0m                 is_causal=is_causal)\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1276\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0min_proj_bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5418\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_separate_proj_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5419\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0min_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is False but in_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5420\u001b[0;31m         \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_in_projection_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5421\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5422\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mq_proj_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"use_separate_proj_weight is True but q_proj_weight is None\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_in_projection_packed\u001b[0;34m(q, k, v, w, b)\u001b[0m\n\u001b[1;32m   4920\u001b[0m             \u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4921\u001b[0m             \u001b[0;31m# reshape to 3, E and not E, 3 is deliberate for better memory coalescing and keeping same order as chunk()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4922\u001b[0;31m             \u001b[0mproj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4923\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproj\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/fx/traceback.py\u001b[0m in \u001b[0;36mformat_stack\u001b[0;34m()\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m# fallback to traceback.format_stack()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraceback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mformat_list\u001b[0;34m(extracted_list)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mwhose\u001b[0m \u001b[0msource\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \"\"\"\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mStackSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextracted_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mformat\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    441\u001b[0m                 frame.filename, frame.lineno, frame.name))\n\u001b[1;32m    442\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m                 \u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'    {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/traceback.py\u001b[0m in \u001b[0;36mline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}