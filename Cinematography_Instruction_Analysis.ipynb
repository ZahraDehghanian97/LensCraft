{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZahraDehghanian97/LenseCraft/blob/master/Cinematography_Instruction_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI_pTggHYrwl"
      },
      "source": [
        "#Imports and Helper Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlSRuyBKY64_"
      },
      "source": [
        "##Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqHtkNVtxah8"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import math\n",
        "from enum import Enum\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpUEXi1jxsMW",
        "outputId": "a68e4510-dd08-42ee-ecc4-15af806d813a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1VT2XfBj9LFWLUBjv65dzC4bVzH0zdNDU\n",
            "From (redirected): https://drive.google.com/uc?id=1VT2XfBj9LFWLUBjv65dzC4bVzH0zdNDU&confirm=t&uuid=1e4c29b9-0ef5-4191-8e9a-ad1390da2b81\n",
            "To: /content/random_simulation_dataset.json\n",
            "100% 105M/105M [00:00<00:00, 146MB/s] \n"
          ]
        }
      ],
      "source": [
        "!gdown -O /content/random_simulation_dataset.json 1VT2XfBj9LFWLUBjv65dzC4bVzH0zdNDU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gbuy_qhbY9_D"
      },
      "source": [
        "##Helpers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR_z1bzx1YRk"
      },
      "outputs": [],
      "source": [
        "class CameraMovementType(Enum):\n",
        "    panLeft = \"panLeft\"\n",
        "    panRight = \"panRight\"\n",
        "    tiltUp = \"tiltUp\"\n",
        "    tiltDown = \"tiltDown\"\n",
        "    dollyIn = \"dollyIn\"\n",
        "    dollyOut = \"dollyOut\"\n",
        "    truckLeft = \"truckLeft\"\n",
        "    truckRight = \"truckRight\"\n",
        "    pedestalUp = \"pedestalUp\"\n",
        "    pedestalDown = \"pedestalDown\"\n",
        "    fullZoomIn = \"fullZoomIn\"\n",
        "    fullZoomOut = \"fullZoomOut\"\n",
        "    halfZoomIn = \"halfZoomIn\"\n",
        "    halfZoomOut = \"halfZoomOut\"\n",
        "    shortZoomIn = \"shortZoomIn\"\n",
        "    shortZoomOut = \"shortZoomOut\"\n",
        "    shortArcShotLeft = \"shortArcShotLeft\"\n",
        "    shortArcShotRight = \"shortArcShotRight\"\n",
        "    halfArcShotLeft = \"halfArcShotLeft\"\n",
        "    halfArcShotRight = \"halfArcShotRight\"\n",
        "    fullArcShotLeft = \"fullArcShotLeft\"\n",
        "    fullArcShotRight = \"fullArcShotRight\"\n",
        "    panAndTilt = \"panAndTilt\"\n",
        "    dollyAndPan = \"dollyAndPan\"\n",
        "    zoomAndTruck = \"zoomAndTruck\"\n",
        "\n",
        "class EasingType(Enum):\n",
        "    linear = \"linear\"\n",
        "    easeInQuad = \"easeInQuad\"\n",
        "    easeInCubic = \"easeInCubic\"\n",
        "    easeInQuart = \"easeInQuart\"\n",
        "    easeInQuint = \"easeInQuint\"\n",
        "    easeOutQuad = \"easeOutQuad\"\n",
        "    easeOutCubic = \"easeOutCubic\"\n",
        "    easeOutQuart = \"easeOutQuart\"\n",
        "    easeOutQuint = \"easeOutQuint\"\n",
        "    easeInOutQuad = \"easeInOutQuad\"\n",
        "    easeInOutCubic = \"easeInOutCubic\"\n",
        "    easeInOutQuart = \"easeInOutQuart\"\n",
        "    easeInOutQuint = \"easeInOutQuint\"\n",
        "    easeInSine = \"easeInSine\"\n",
        "    easeOutSine = \"easeOutSine\"\n",
        "    easeInOutSine = \"easeInOutSine\"\n",
        "    easeInExpo = \"easeInExpo\"\n",
        "    easeOutExpo = \"easeOutExpo\"\n",
        "    easeInOutExpo = \"easeInOutExpo\"\n",
        "    easeInCirc = \"easeInCirc\"\n",
        "    easeOutCirc = \"easeOutCirc\"\n",
        "    easeInOutCirc = \"easeInOutCirc\"\n",
        "    easeInBounce = \"easeInBounce\"\n",
        "    easeOutBounce = \"easeOutBounce\"\n",
        "    easeInOutBounce = \"easeInOutBounce\"\n",
        "    easeInElastic = \"easeInElastic\"\n",
        "    easeOutElastic = \"easeOutElastic\"\n",
        "    easeInOutElastic = \"easeInOutElastic\"\n",
        "\n",
        "class CameraAngle(Enum):\n",
        "    lowAngle = \"lowAngle\"\n",
        "    mediumAngle = \"mediumAngle\"\n",
        "    highAngle = \"highAngle\"\n",
        "    birdsEyeView = \"birdsEyeView\"\n",
        "\n",
        "class ShotType(Enum):\n",
        "    closeUp = \"closeUp\"\n",
        "    mediumShot = \"mediumShot\"\n",
        "    longShot = \"longShot\"\n",
        "\n",
        "\n",
        "movement_descriptions = {\n",
        "    \"panLeft\": \"panning left\",\n",
        "    \"panRight\": \"panning right\",\n",
        "    \"tiltUp\": \"tilting up\",\n",
        "    \"tiltDown\": \"tilting down\",\n",
        "    \"dollyIn\": \"moving closer\",\n",
        "    \"dollyOut\": \"moving away\",\n",
        "    \"truckLeft\": \"moving left\",\n",
        "    \"truckRight\": \"moving right\",\n",
        "    \"pedestalUp\": \"rising vertically\",\n",
        "    \"pedestalDown\": \"descending vertically\",\n",
        "    \"fullZoomIn\": \"zooming in fully\",\n",
        "    \"fullZoomOut\": \"zooming out fully\",\n",
        "    \"halfZoomIn\": \"zooming in halfway\",\n",
        "    \"halfZoomOut\": \"zooming out halfway\",\n",
        "    \"shortZoomIn\": \"zooming in slightly\",\n",
        "    \"shortZoomOut\": \"zooming out slightly\",\n",
        "    \"shortArcShotLeft\": \"moving in a short arc to the left\",\n",
        "    \"shortArcShotRight\": \"moving in a short arc to the right\",\n",
        "    \"halfArcShotLeft\": \"moving in a half arc to the left\",\n",
        "    \"halfArcShotRight\": \"moving in a half arc to the right\",\n",
        "    \"fullArcShotLeft\": \"moving in a full arc to the left\",\n",
        "    \"fullArcShotRight\": \"moving in a full arc to the right\",\n",
        "    \"panAndTilt\": \"panning and tilting\",\n",
        "    \"dollyAndPan\": \"moving and panning\",\n",
        "    \"zoomAndTruck\": \"zooming and moving sideways\",\n",
        "}\n",
        "\n",
        "easing_descriptions = {\n",
        "    \"linear\": \"at a constant speed\",\n",
        "    \"easeInQuad\": \"slowly at first, then accelerating gradually\",\n",
        "    \"easeInCubic\": \"slowly at first, then accelerating more rapidly\",\n",
        "    \"easeInQuart\": \"very slowly at first, then accelerating dramatically\",\n",
        "    \"easeInQuint\": \"extremely slowly at first, then accelerating very dramatically\",\n",
        "    \"easeOutQuad\": \"quickly at first, then decelerating gradually\",\n",
        "    \"easeOutCubic\": \"quickly at first, then decelerating more rapidly\",\n",
        "    \"easeOutQuart\": \"very quickly at first, then decelerating dramatically\",\n",
        "    \"easeOutQuint\": \"extremely quickly at first, then decelerating very dramatically\",\n",
        "    \"easeInOutQuad\": \"gradually accelerating, then gradually decelerating\",\n",
        "    \"easeInOutCubic\": \"slowly accelerating, then decelerating more rapidly\",\n",
        "    \"easeInOutQuart\": \"slowly accelerating, then decelerating dramatically\",\n",
        "    \"easeInOutQuint\": \"very slowly accelerating, then decelerating very dramatically\",\n",
        "    \"easeInSine\": \"with a gentle start, gradually increasing in speed\",\n",
        "    \"easeOutSine\": \"quickly at first, then gently decelerating\",\n",
        "    \"easeInOutSine\": \"with a gentle start and end, faster in the middle\",\n",
        "    \"easeInExpo\": \"starting very slowly, then accelerating exponentially\",\n",
        "    \"easeOutExpo\": \"starting very fast, then decelerating exponentially\",\n",
        "    \"easeInOutExpo\": \"starting and ending slowly, with rapid acceleration and deceleration in the middle\",\n",
        "    \"easeInCirc\": \"starting slowly, then accelerating sharply towards the end\",\n",
        "    \"easeOutCirc\": \"starting quickly, then decelerating sharply towards the end\",\n",
        "    \"easeInOutCirc\": \"with sharp acceleration and deceleration at both ends\",\n",
        "    \"easeInBounce\": \"with a bouncing effect that intensifies towards the end\",\n",
        "    \"easeOutBounce\": \"quickly at first, then bouncing to a stop\",\n",
        "    \"easeInOutBounce\": \"with a bouncing effect at both the start and end\",\n",
        "    \"easeInElastic\": \"with an elastic effect that intensifies towards the end\",\n",
        "    \"easeOutElastic\": \"quickly at first, then oscillating to a stop\",\n",
        "    \"easeInOutElastic\": \"with an elastic effect at both the start and end\",\n",
        "}\n",
        "\n",
        "angle_descriptions = {\n",
        "    \"lowAngle\": \"from a low angle\",\n",
        "    \"mediumAngle\": \"from a medium angle\",\n",
        "    \"highAngle\": \"from a high angle\",\n",
        "    \"birdsEyeView\": \"from a bird's eye view\",\n",
        "}\n",
        "\n",
        "shot_descriptions = {\n",
        "    \"closeUp\": \"in a close-up shot\",\n",
        "    \"mediumShot\": \"in a medium shot\",\n",
        "    \"longShot\": \"in a long shot\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dT895usiy-v9"
      },
      "outputs": [],
      "source": [
        "def get_clip_embedding(text: str) -> torch.Tensor:\n",
        "    with torch.no_grad():\n",
        "        inputs = clip_tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        embedding = clip_text_encoder(**inputs).pooler_output\n",
        "    return embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-Ag9MugZJwF"
      },
      "outputs": [],
      "source": [
        "def get_movement_description(movement, easing, camera_angle=None, shot_type=None):\n",
        "    description = f\"The camera is {movement_descriptions.get(movement, movement)}\"\n",
        "\n",
        "    if easing in easing_descriptions:\n",
        "        description += f\" {easing_descriptions[easing]}\"\n",
        "    else:\n",
        "        description += f\" with {easing} easing\"\n",
        "\n",
        "    if camera_angle:\n",
        "        description += f\", {angle_descriptions.get(camera_angle, camera_angle)}\"\n",
        "\n",
        "    if shot_type:\n",
        "        description += f\" {shot_descriptions.get(shot_type, shot_type)}\"\n",
        "\n",
        "    return description\n",
        "\n",
        "def get_all_movement_descriptions():\n",
        "    descriptions = []\n",
        "    for movement in CameraMovementType:\n",
        "        for easing in EasingType:\n",
        "            descriptions.append(get_movement_description(movement.value, easing.value))\n",
        "            for shot in ShotType:\n",
        "                descriptions.append(get_movement_description(movement.value, easing.value, shot_type=shot.value))\n",
        "            for angle in CameraAngle:\n",
        "                descriptions.append(get_movement_description(movement.value, easing.value, camera_angle=angle.value))\n",
        "                for shot in ShotType:\n",
        "                    descriptions.append(get_movement_description(movement.value, easing.value, camera_angle=angle.value, shot_type=shot.value))\n",
        "    return descriptions\n",
        "\n",
        "def get_movement_index(movement, easing, camera_angle=None, shot_type=None):\n",
        "    description = get_movement_description(movement, easing, camera_angle, shot_type)\n",
        "    return all_movement_descriptions.index(description)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_negative_indices(positive_indices, num_features, num_negatives):\n",
        "    batch_size = positive_indices.size(0)\n",
        "    all_indices = torch.arange(num_features, device=positive_indices.device).unsqueeze(0).expand(batch_size, -1)\n",
        "    mask = all_indices != positive_indices.unsqueeze(1)\n",
        "    possible_negatives = all_indices[mask].view(batch_size, num_features - 1)\n",
        "    indices = torch.randint(0, num_features - 1, (batch_size, num_negatives), device=positive_indices.device)\n",
        "    negative_indices = torch.gather(possible_negatives, 1, indices)\n",
        "    return negative_indices\n",
        "\n",
        "\n",
        "def get_text_features(clip_text_features, positive_indices, negative_indices):\n",
        "    positive_features = clip_text_features[positive_indices].unsqueeze(1)\n",
        "    negative_features = clip_text_features[negative_indices.view(-1)].view(len(positive_indices), -1, clip_text_features.size(1))\n",
        "    return torch.cat([positive_features, negative_features], dim=1)\n",
        "\n",
        "def compute_similarities(text_features, latent, temperature):\n",
        "    return torch.bmm(text_features, latent.unsqueeze(2)).squeeze(2) / temperature\n",
        "\n",
        "def contrastive_loss(latent, clip_text_features, positive_indices, temperature=0.3, num_negatives=50):\n",
        "    latent = F.normalize(latent, p=2, dim=1)\n",
        "    clip_text_features = F.normalize(clip_text_features, p=2, dim=1)\n",
        "\n",
        "    negative_indices = get_negative_indices(positive_indices, clip_text_features.size(0), num_negatives)\n",
        "    text_features = get_text_features(clip_text_features, positive_indices, negative_indices)\n",
        "    similarities = compute_similarities(text_features, latent, temperature)\n",
        "\n",
        "    targets = torch.zeros(latent.size(0), dtype=torch.long, device=latent.device)\n",
        "    return F.cross_entropy(similarities, targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKcsSyQYy6AN"
      },
      "outputs": [],
      "source": [
        "def normalize_camera_trajectory(camera_frames, subject_center, subject_area):\n",
        "    trajectories = []\n",
        "    for frame in camera_frames:\n",
        "        trajectory = []\n",
        "        position = np.array([frame['position']['x'], frame['position']['y'], frame['position']['z']])\n",
        "        relative_position = (position - subject_center) * subject_area\n",
        "        trajectory.extend(relative_position.tolist())\n",
        "        trajectory.extend([0, 0, 0])  # trajectory.extend([frame['angle']['x'], frame['angle']['y'], frame['angle']['z']])\n",
        "        trajectory.append(frame['focalLength'])\n",
        "        trajectories.append(trajectory)\n",
        "    return trajectories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u621kky1Zm8u"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "def process_camera_trajectory(trajectory: torch.Tensor) -> List[Dict]:\n",
        "    trajectory = trajectory.cpu().detach().numpy()\n",
        "    camera_frames = []\n",
        "    for frame in trajectory:\n",
        "        position, angle, focal_length = frame[:3], frame[3:6], frame[6]\n",
        "        camera_frame = {\n",
        "            \"position\": {axis: float(value) for axis, value in zip(['x', 'y', 'z'], position)},\n",
        "            \"angle\": {axis: float(value) for axis, value in zip(['x', 'y', 'z'], angle)},\n",
        "            \"focalLength\": float(focal_length)\n",
        "        }\n",
        "        camera_frames.append(camera_frame)\n",
        "    return camera_frames\n",
        "\n",
        "def save_to_json(trajectory: torch.Tensor, filename: str):\n",
        "    camera_frames = process_camera_trajectory(trajectory)\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(camera_frames, f, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsJo1-aVZaSK"
      },
      "source": [
        "#Implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU3OzBiKt2dt",
        "outputId": "790f5429-9479-406f-9b93-ecb760801966"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "clip_model_name = \"openai/clip-vit-large-patch14\" #@param [\"openai/clip-vit-base-patch32\", \"openai/clip-vit-large-patch14\"]\n",
        "\n",
        "if clip_model_name == \"openai/clip-vit-large-patch14\":\n",
        "    clip_embedding_dim = 768\n",
        "else:\n",
        "    clip_embedding_dim = 512\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_tokenizer = CLIPTokenizer.from_pretrained(clip_model_name)\n",
        "clip_text_encoder = CLIPTextModel.from_pretrained(clip_model_name).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGc3_s9r1h9s",
        "outputId": "8ad82ca7-db25-41e0-be4f-ccb7595ce4aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing batches: 100%|██████████| 140/140 [00:29<00:00,  4.73it/s]\n"
          ]
        }
      ],
      "source": [
        "all_movement_descriptions = get_all_movement_descriptions()\n",
        "\n",
        "batch_size = 100\n",
        "all_clip_text_features = []\n",
        "\n",
        "for i in tqdm(range(0, len(all_movement_descriptions), batch_size), desc=\"Processing batches\"):\n",
        "    batch = all_movement_descriptions[i:i+batch_size]\n",
        "    batch_features = get_clip_embedding(batch)\n",
        "    all_clip_text_features.append(batch_features)\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "all_clip_text_features = torch.cat(all_clip_text_features, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yArArZxWxMMj"
      },
      "outputs": [],
      "source": [
        "class SimulationDataset(Dataset):\n",
        "    def __init__(self, json_file_path: str):\n",
        "        with open(json_file_path, 'r') as file:\n",
        "            raw_data = json.load(file)\n",
        "        self.simulation_data = [self._process_single_simulation(sim) for sim in raw_data['simulations']\n",
        "                if self._is_simulation_valid(sim)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.simulation_data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        simulation = self.simulation_data[index]\n",
        "        return {\n",
        "            'camera_trajectory': torch.tensor(simulation['camera_trajectory'], dtype=torch.float32),\n",
        "            'movement_type': torch.tensor(simulation['movement_type'], dtype=torch.long),\n",
        "            'easing_type': torch.tensor(simulation['easing_type'], dtype=torch.long),\n",
        "            'label_index': simulation['label_index']\n",
        "        }\n",
        "\n",
        "    def _is_simulation_valid(self, simulation):\n",
        "        return (len(simulation['instructions']) == 1 and\n",
        "                simulation['instructions'][0]['frameCount'] == 30 and\n",
        "                len(simulation['cameraFrames']) == 30)\n",
        "\n",
        "    def _process_single_simulation(self, simulation):\n",
        "        instruction = simulation['instructions'][0]\n",
        "        subject = simulation['subjects'][0]\n",
        "        subject_center = np.array([subject['position']['x'], subject['position']['y'], subject['position']['z']])\n",
        "\n",
        "        camera_trajectory = normalize_camera_trajectory(simulation['cameraFrames'], subject_center, subject_area=1)\n",
        "\n",
        "        movement_type = CameraMovementType[instruction['cameraMovement']].value\n",
        "        easing_type = EasingType[instruction['movementEasing']].value\n",
        "        camera_angle = CameraAngle[instruction.get('initialCameraAngle')].value if 'initialCameraAngle' in instruction else None\n",
        "        shot_type = ShotType[instruction.get('initialShotType')].value if 'initialShotType' in instruction else None\n",
        "\n",
        "        movement_type_index = list(CameraMovementType).index(CameraMovementType(movement_type))\n",
        "        easing_type_index = list(EasingType).index(EasingType(easing_type))\n",
        "\n",
        "        label_index = get_movement_index(movement_type, easing_type, camera_angle, shot_type)\n",
        "\n",
        "        return {\n",
        "            'camera_trajectory': camera_trajectory,\n",
        "            'movement_type': movement_type_index,\n",
        "            'easing_type': easing_type_index,\n",
        "            'label_index': label_index\n",
        "        }\n",
        "\n",
        "def batch_collate(batch):\n",
        "    return {\n",
        "        'camera_trajectory': torch.stack([item['camera_trajectory'] for item in batch]),\n",
        "        'movement_type': torch.stack([item['movement_type'] for item in batch]),\n",
        "        'easing_type': torch.stack([item['easing_type'] for item in batch]),\n",
        "        'positive_indices': torch.tensor([item['label_index'] for item in batch], dtype=torch.long)\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pu60D7QlzDJR"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length, dropout_rate=0.1):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout_rate)\n",
        "\n",
        "        position = torch.arange(max_seq_length).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_seq_length, 1, d_model)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        return self.dropout(x)\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, max_seq_length, latent_dim, dropout_rate):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        self.input_projection = nn.Linear(input_dim[1], d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length, dropout_rate)\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout_rate)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers)\n",
        "\n",
        "        self.latent_projection = nn.Linear(d_model * max_seq_length, latent_dim)\n",
        "\n",
        "    def forward(self, src, mask=None):\n",
        "        src = self.input_projection(src)\n",
        "        src = src.permute(1, 0, 2)\n",
        "        src = self.pos_encoder(src)\n",
        "\n",
        "        if mask is None:\n",
        "            mask = torch.ones((src.size(1), src.size(0))).to(src.device)\n",
        "\n",
        "        memory = self.transformer_encoder(src, src_key_padding_mask=mask)\n",
        "\n",
        "        latent = self.latent_projection(memory.permute(1, 0, 2).reshape(memory.size(1), -1))\n",
        "\n",
        "        return latent\n",
        "\n",
        "class AutoregressiveDecoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_decoder_layers, max_seq_length, latent_dim, output_dim, dropout_rate):\n",
        "        super(AutoregressiveDecoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        self.latent_projection = nn.Linear(latent_dim, d_model)\n",
        "        self.tgt_projection = nn.Linear(output_dim, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dropout=dropout_rate)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.output_projection = nn.Linear(d_model, output_dim)\n",
        "\n",
        "    def forward(self, tgt, latent, tgt_mask=None):\n",
        "        memory = self.latent_projection(latent).unsqueeze(1)\n",
        "        tgt = self.tgt_projection(tgt)\n",
        "        tgt = self.pos_encoder(tgt.transpose(0, 1))\n",
        "\n",
        "        if tgt_mask is None:\n",
        "            tgt_mask = generate_square_subsequent_mask(tgt.size(0)).to(tgt.device)\n",
        "\n",
        "        output = self.transformer_decoder(tgt, memory.transpose(0, 1), tgt_mask=tgt_mask)\n",
        "        output = self.output_projection(output.transpose(0, 1))\n",
        "        return output\n",
        "\n",
        "\n",
        "class SingleStageDecoder(nn.Module):\n",
        "    def __init__(self, d_model, nhead, num_decoder_layers, max_seq_length, latent_dim, output_dim, dropout_rate):\n",
        "        super(SingleStageDecoder, self).__init__()\n",
        "        self.d_model = d_model\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        self.latent_projection = nn.Linear(latent_dim, d_model * max_seq_length)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        decoder_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead, dropout=dropout_rate)\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)\n",
        "\n",
        "        self.output_projection = nn.Linear(d_model, output_dim)\n",
        "\n",
        "    def forward(self, latent):\n",
        "        memory = self.latent_projection(latent).view(-1, self.max_seq_length, self.d_model).transpose(0, 1)\n",
        "        memory = self.pos_encoder(memory)\n",
        "\n",
        "        tgt = torch.zeros_like(memory)\n",
        "        output = self.transformer_decoder(tgt, memory)\n",
        "        output = self.output_projection(output.transpose(0, 1))\n",
        "        return output\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, latent_dim, num_movement_types, num_easing_types, dropout_rate=0.1):\n",
        "        super(Classifier, self).__init__()\n",
        "        hidden_dim = 256\n",
        "\n",
        "        self.movement_type_classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, num_movement_types)\n",
        "        )\n",
        "\n",
        "        self.easing_type_classifier = nn.Sequential(\n",
        "            nn.Linear(latent_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, num_easing_types)\n",
        "        )\n",
        "\n",
        "    def forward(self, latent):\n",
        "        movement_type_logits = self.movement_type_classifier(latent)\n",
        "        easing_type_logits = self.easing_type_classifier(latent)\n",
        "        return movement_type_logits, easing_type_logits\n",
        "\n",
        "\n",
        "class MultiTaskAutoencoder(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, num_movement_types, num_easing_types,\n",
        "                 max_seq_length, latent_dim, dropout_rate, decoder_type='single_stage'):\n",
        "        super(MultiTaskAutoencoder, self).__init__()\n",
        "        self.encoder = Encoder(input_dim, d_model, nhead, num_encoder_layers, max_seq_length, latent_dim, dropout_rate)\n",
        "        self.decoder_type = decoder_type\n",
        "        self.classifier = Classifier(latent_dim, num_movement_types, num_easing_types, dropout_rate=dropout_rate)\n",
        "        self.max_seq_length = max_seq_length\n",
        "\n",
        "        if decoder_type == 'autoregressive':\n",
        "            self.decoder = AutoregressiveDecoder(d_model, nhead, num_decoder_layers, max_seq_length, latent_dim, input_dim[1], dropout_rate)\n",
        "        elif decoder_type == 'single_stage':\n",
        "            self.decoder = SingleStageDecoder(d_model, nhead, num_decoder_layers, max_seq_length, latent_dim, input_dim[1], dropout_rate)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid decoder_type. Choose 'autoregressive' or 'single_stage'.\")\n",
        "\n",
        "    def forward(self, src: torch.Tensor, mask: torch.Tensor = None):\n",
        "        latent = self.encoder(src, mask)\n",
        "\n",
        "        movement_type_logits, easing_type_logits = self.classifier(latent)\n",
        "\n",
        "        if self.decoder_type == 'autoregressive':\n",
        "            reconstructed = self.autoregressive_decode(latent, src[:, 0, :], src)\n",
        "        else:\n",
        "            reconstructed = self.decoder(latent)\n",
        "\n",
        "        return {\n",
        "            'latent': latent,\n",
        "            'movement_type_logits': movement_type_logits,\n",
        "            'easing_type_logits': easing_type_logits,\n",
        "            'reconstructed': reconstructed,\n",
        "        }\n",
        "\n",
        "    def autoregressive_decode(self, latent: torch.Tensor, initial_input: torch.Tensor=None, guidance: torch.Tensor=None):\n",
        "        if initial_input is None:\n",
        "            initial_input = torch.full((latent.shape[0], 1, 7), 10, device=device)\n",
        "        else:\n",
        "            initial_input = initial_input.unsqueeze(1)\n",
        "\n",
        "        output_sequence = [initial_input]\n",
        "\n",
        "        for i in range(1, self.max_seq_length):\n",
        "            if guidance is not None:\n",
        "                guided_tgt = guidance[:, :i+1, :]\n",
        "            else:\n",
        "                guided_tgt = torch.cat(output_sequence, dim=1)\n",
        "\n",
        "            tgt_mask = generate_square_subsequent_mask(i+1).to(device)\n",
        "\n",
        "            output = self.decoder(guided_tgt, latent, tgt_mask)\n",
        "            next_token = output[:, -1:, :]\n",
        "\n",
        "            output_sequence.append(next_token)\n",
        "\n",
        "        return torch.cat(output_sequence, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2OsT98BzXKy"
      },
      "outputs": [],
      "source": [
        "def init_losses():\n",
        "  return {\n",
        "      'total': 0,\n",
        "      'movement_type': 0,\n",
        "      'easing_type': 0,\n",
        "      'reconstruction': 0,\n",
        "      'clip': 0,\n",
        "      'clip_contrastive': 0\n",
        "  }\n",
        "\n",
        "def print_detailed_losses(phase, losses):\n",
        "    print(f\"{phase} Losses - \"\n",
        "          f\"Movement: {losses['movement_type']:.4f}, \"\n",
        "          f\"Easing: {losses['easing_type']:.4f}, \"\n",
        "          f\"Reconstruction: {losses['reconstruction']:.4f}, \"\n",
        "          f\"CLIP: {losses['clip']:.4f}, \"\n",
        "          f\"CLIP Contrastive: {losses['clip_contrastive']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eglTw5WODOIO"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, dataloader, optimizer, criterion, noise_std=0.0, mask_ratio=0.0):\n",
        "    model.train()\n",
        "    total_losses = init_losses()\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
        "        losses = process_batch(model, batch, criterion, noise_std, mask_ratio)\n",
        "\n",
        "        loss = sum(losses.values())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for key in total_losses:\n",
        "            total_losses[key] += losses[key].item() if torch.is_tensor(losses[key]) else losses[key]\n",
        "\n",
        "    return {k: v / len(dataloader) for k, v in total_losses.items()}\n",
        "\n",
        "def validate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_losses = init_losses()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
        "            losses = process_batch(model, batch, criterion)\n",
        "            loss = sum(losses.values())\n",
        "\n",
        "            for key in total_losses:\n",
        "                total_losses[key] += losses[key].item() if torch.is_tensor(losses[key]) else losses[key]\n",
        "\n",
        "    return {k: v / len(dataloader) for k, v in total_losses.items()}\n",
        "\n",
        "\n",
        "def process_batch(model, batch, criterion, noise_std=0.0, mask_ratio=0.0):\n",
        "    camera_trajectory = batch['camera_trajectory'].to(device)\n",
        "    movement_type = batch['movement_type'].to(device)\n",
        "    easing_type = batch['easing_type'].to(device)\n",
        "    positive_indices = batch['positive_indices'].to(device)\n",
        "\n",
        "    mask = torch.bernoulli(torch.full((camera_trajectory.shape[0], camera_trajectory.shape[1]), mask_ratio, device=device)).bool()\n",
        "    noise = torch.normal(mean=0, std=noise_std, size=camera_trajectory.shape, device=device)\n",
        "    output = model(camera_trajectory + noise, mask)\n",
        "\n",
        "    losses = {\n",
        "        'movement_type': criterion['classification'](output['movement_type_logits'], movement_type),\n",
        "        'easing_type': criterion['classification'](output['easing_type_logits'], easing_type),\n",
        "        'reconstruction': (criterion['reconstruction'](output['reconstructed'], camera_trajectory)) * 2,\n",
        "        'clip': (1 - F.cosine_similarity(output['latent'], all_clip_text_features[positive_indices]).mean()) * 5,\n",
        "        'clip_contrastive': (contrastive_loss(output['latent'], all_clip_text_features, positive_indices)) * 0.3\n",
        "    }\n",
        "    losses['total'] = sum(losses.values())\n",
        "\n",
        "    return losses\n",
        "\n",
        "def train_model(model, train_dataloader, val_dataloader, config):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config['learning_rate'])\n",
        "    criterion = {\n",
        "        'classification': nn.CrossEntropyLoss(),\n",
        "        'reconstruction': nn.MSELoss()\n",
        "    }\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(config['num_epochs']):\n",
        "        train_losses = train_epoch(model, train_dataloader, optimizer, criterion, config['noise_std'], config['mask_ratio'])\n",
        "        val_losses = validate(model, val_dataloader, criterion)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{config['num_epochs']}\")\n",
        "        print(f\"Train Loss: {train_losses['total']:.4f}, Validation Loss: {val_losses['total']:.4f}\")\n",
        "        print_detailed_losses(\"Train\", train_losses)\n",
        "        print_detailed_losses(\"Valid\", val_losses)\n",
        "\n",
        "        if val_losses['total'] < best_val_loss:\n",
        "            best_val_loss = val_losses['total']\n",
        "            epochs_without_improvement = 0\n",
        "            torch.save(model.state_dict(), 'best_model.pth')\n",
        "            print(\"New best model saved!\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        if epochs_without_improvement >= config['patience']:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GPK8Tp53m8d"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dataset = SimulationDataset('random_simulation_dataset.json')\n",
        "\n",
        "unused_data, used_data = train_test_split(dataset, test_size=0.1, random_state=42)\n",
        "train_dataset, val_dataset = train_test_split(used_data, test_size=0.3, random_state=42)\n",
        "\n",
        "batch_size = 8\n",
        "num_workers = 2\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                              collate_fn=batch_collate, num_workers=num_workers)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n",
        "                            collate_fn=batch_collate, num_workers=num_workers)\n",
        "\n",
        "input_dim = [30, 7]  # 30 camera_trajectory, 7 values per frame\n",
        "d_model = 256\n",
        "nhead = 8\n",
        "num_encoder_layers = 3\n",
        "num_decoder_layers = 3\n",
        "max_seq_length = 30\n",
        "dropout_rate = 0\n",
        "\n",
        "num_movement_types = len(CameraMovementType)\n",
        "num_easing_types = len(EasingType)\n",
        "latent_dim = clip_embedding_dim\n",
        "\n",
        "model = MultiTaskAutoencoder(input_dim, d_model, nhead, num_encoder_layers, num_decoder_layers, num_movement_types, num_easing_types,\n",
        "                             max_seq_length, latent_dim, dropout_rate).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B8zCiaVe3ltM"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'num_epochs': 100,\n",
        "    'patience': 10,\n",
        "    'learning_rate': 0.001,\n",
        "    'noise_std': 0.0,\n",
        "    'mask_ratio': 0.0\n",
        "}\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "train_model(model, train_dataloader, val_dataloader, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONMw8qWQkKqN"
      },
      "source": [
        "##Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp5tXDnz02Uj"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "model.eval()\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Il2pEix4xQEv"
      },
      "outputs": [],
      "source": [
        "file_path = '/content/result.json'\n",
        "\n",
        "input_text = \"The camera is moving in a full arc to the left with a bouncing effect that intensifies towards the end in a close-up shot\" # The camera is panning left at a constant speed, from a low angle in a close-up shot\n",
        "latent = get_clip_embedding(input_text)\n",
        "camera_frames = model.autoregressive_decode(latent).squeeze(0)\n",
        "save_to_json(camera_frames, file_path)\n",
        "\n",
        "files.download(file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oEKo1EUbn25b"
      },
      "outputs": [],
      "source": [
        "input_path = '/content/input.json'\n",
        "output_path = '/content/output.json'\n",
        "\n",
        "input_camera_tranjectory = dataset[3]['camera_trajectory'].to(device)\n",
        "reconstructed_camera_tranjectory = model(input_camera_tranjectory.unsqueeze(0))['reconstructed'].squeeze(0)\n",
        "\n",
        "save_to_json(input_camera_tranjectory, input_path)\n",
        "save_to_json(reconstructed_camera_tranjectory, output_path)\n",
        "\n",
        "files.download(input_path)\n",
        "files.download(output_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
