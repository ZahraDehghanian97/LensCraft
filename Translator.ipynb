{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pV92-mAhWGHv",
        "shPFPtMnTyJU",
        "D5d-0EbEWD2m",
        "-Uh89mIWldDL"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ZahraDehghanian97/LensCraft/blob/master/Translator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Generation (Using Templates)"
      ],
      "metadata": {
        "id": "shPFPtMnTyJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "CAMERA_PARAMETERS = {\n",
        "    \"CameraVerticalAngle\": {\n",
        "        \"values\": [\"low\", \"eye\", \"high\", \"overhead\", \"birdsEye\"],\n",
        "        \"synonyms\": {\n",
        "            \"low\": [\"low-angle\", \"from below\", \"upward-facing\", \"worm's eye view\", \"ground level\", \"looking up\"],\n",
        "            \"eye\": [\"eye-level\", \"neutral\", \"straight-on\", \"level view\", \"natural angle\", \"standard height\"],\n",
        "            \"high\": [\"high-angle\", \"from above\", \"downward-facing\", \"elevated view\", \"raised perspective\", \"looking down\"],\n",
        "            \"overhead\": [\"overhead\", \"from directly above\", \"top-down\", \"ceiling view\", \"direct overhead\", \"vertical down\"],\n",
        "            \"birdsEye\": [\"bird's eye\", \"aerial\", \"far overhead\", \"extreme overhead\", \"elevated overhead\", \"sky view\"]\n",
        "        }\n",
        "    },\n",
        "    \"ShotSize\": {\n",
        "        \"values\": [\n",
        "            \"extremeCloseUp\",\n",
        "            \"closeUp\",\n",
        "            \"mediumCloseUp\",\n",
        "            \"mediumShot\",\n",
        "            \"fullShot\",\n",
        "            \"longShot\",\n",
        "            \"veryLongShot\",\n",
        "            \"extremeLongShot\",\n",
        "        ],\n",
        "        \"synonyms\": {\n",
        "            \"extremeCloseUp\": [\"extreme close-up\", \"macro shot\", \"detail view\", \"intimate detail\", \"super close-up\", \"microscopic view\"],\n",
        "            \"closeUp\": [\"close-up\", \"tight shot\", \"near view\", \"facial shot\", \"intimate frame\", \"detailed view\"],\n",
        "            \"mediumCloseUp\": [\"medium close-up\", \"head and shoulders\", \"bust shot\", \"upper body frame\", \"shoulder shot\", \"partial upper body\"],\n",
        "            \"mediumShot\": [\"medium shot\", \"mid-shot\", \"waist shot\", \"half body\", \"waist-up view\", \"mid-frame\"],\n",
        "            \"fullShot\": [\"full shot\", \"full body\", \"head to toe\", \"complete view\", \"entire figure\", \"full frame\"],\n",
        "            \"longShot\": [\"long shot\", \"wide shot\", \"full view\", \"environmental view\", \"contextual shot\", \"scene-setting shot\"],\n",
        "            \"veryLongShot\": [\"very long shot\", \"very wide shot\", \"establishing shot\", \"master shot\", \"broad view\", \"expansive shot\"],\n",
        "            \"extremeLongShot\": [\"extreme long shot\", \"extreme wide\", \"panoramic view\", \"vista shot\", \"grand view\", \"epic scale\"]\n",
        "        }\n",
        "    },\n",
        "    \"MovementSpeed\": {\n",
        "        \"values\": [\n",
        "            \"slowToFast\",\n",
        "            \"fastToSlow\",\n",
        "            \"constant\",\n",
        "            \"stopAndGo\",\n",
        "            \"deliberateStartStop\",\n",
        "            \"erratic\",\n",
        "            \"pulsing\"\n",
        "        ],\n",
        "        \"synonyms\": {\n",
        "            \"slowToFast\": [\"gradually accelerating\", \"increasing speed\", \"picking up pace\", \"building momentum\", \"ramping up\", \"progressive acceleration\"],\n",
        "            \"fastToSlow\": [\"gradually decelerating\", \"decreasing speed\", \"slowing down\", \"easing to stop\", \"winding down\", \"tapering speed\"],\n",
        "            \"constant\": [\"steady\", \"uniform\", \"consistent speed\", \"unchanging pace\", \"maintained velocity\", \"even movement\"],\n",
        "            \"stopAndGo\": [\"intermittent\", \"start and stop\", \"punctuated movement\", \"staccato motion\", \"interrupted flow\", \"periodic pauses\"],\n",
        "            \"deliberateStartStop\": [\"measured pauses\", \"intentional stops\", \"rhythmic stopping\", \"choreographed pauses\", \"planned breaks\", \"controlled stops\"],\n",
        "            \"erratic\": [\"unpredictable\", \"varying\", \"irregular\", \"random speeds\", \"sporadic\", \"changeable pace\"],\n",
        "            \"pulsing\": [\"rhythmic\", \"beating\", \"oscillating\", \"cyclic motion\", \"wave-like\", \"periodic\"]\n",
        "        }\n",
        "    },\n",
        "    \"SubjectInFramePosition\": {\n",
        "        \"values\": [\n",
        "            \"left\", \"right\", \"top\", \"bottom\", \"center\",\n",
        "            \"topLeft\", \"topRight\", \"bottomLeft\", \"bottomRight\",\n",
        "            \"outerLeft\", \"outerRight\", \"outerTop\", \"outerBottom\",\n",
        "            \"offsetCenter\",\n",
        "            \"diagonal\"\n",
        "        ],\n",
        "        \"synonyms\": {\n",
        "            \"left\": [\"on the left\", \"left side\", \"left portion\", \"leftward\", \"port side\", \"left zone\"],\n",
        "            \"right\": [\"on the right\", \"right side\", \"right portion\", \"rightward\", \"starboard side\", \"right zone\"],\n",
        "            \"top\": [\"at the top\", \"upper portion\", \"top area\", \"upper zone\", \"superior position\", \"high position\"],\n",
        "            \"bottom\": [\"at the bottom\", \"lower portion\", \"bottom area\", \"lower zone\", \"inferior position\", \"low position\"],\n",
        "            \"center\": [\"in the center\", \"middle\", \"central position\", \"dead center\", \"bull's eye\", \"epicenter\"],\n",
        "            \"topLeft\": [\"upper left\", \"top left corner\", \"northwest position\", \"high left\", \"upper port\", \"northeast corner\"],\n",
        "            \"topRight\": [\"upper right\", \"top right corner\", \"northeast position\", \"high right\", \"upper starboard\", \"northwest corner\"],\n",
        "            \"bottomLeft\": [\"lower left\", \"bottom left corner\", \"southwest position\", \"low left\", \"lower port\", \"southwest corner\"],\n",
        "            \"bottomRight\": [\"lower right\", \"bottom right corner\", \"southeast position\", \"low right\", \"lower starboard\", \"southeast corner\"],\n",
        "            \"outerLeft\": [\"far left\", \"leftmost edge\", \"extreme left\", \"peripheral left\", \"margin left\", \"border left\"],\n",
        "            \"outerRight\": [\"far right\", \"rightmost edge\", \"extreme right\", \"peripheral right\", \"margin right\", \"border right\"],\n",
        "            \"outerTop\": [\"very top\", \"topmost edge\", \"extreme top\", \"peripheral top\", \"margin top\", \"border top\"],\n",
        "            \"outerBottom\": [\"very bottom\", \"bottommost edge\", \"extreme bottom\", \"peripheral bottom\", \"margin bottom\", \"border bottom\"],\n",
        "            \"offsetCenter\": [\"slightly off-center\", \"near center\", \"just off middle\", \"asymmetric center\", \"shifted center\", \"biased center\"],\n",
        "            \"diagonal\": [\"diagonal position\", \"angular placement\", \"oblique position\", \"slanted position\", \"diagonal offset\", \"cross-frame\"]\n",
        "        }\n",
        "    },\n",
        "    \"SubjectView\": {\n",
        "        \"values\": [\n",
        "            \"front\",\n",
        "            \"back\",\n",
        "            \"left\",\n",
        "            \"right\",\n",
        "            \"threeQuarterFrontLeft\",\n",
        "            \"threeQuarterFrontRight\",\n",
        "            \"threeQuarterBackLeft\",\n",
        "            \"threeQuarterBackRight\",\n",
        "            \"overhead\",\n",
        "            \"silhouette\"\n",
        "        ],\n",
        "        \"synonyms\": {\n",
        "            \"front\": [\"front view\", \"facing camera\", \"direct front\", \"head-on\", \"straight ahead\", \"forward facing\"],\n",
        "            \"back\": [\"back view\", \"from behind\", \"rear view\", \"posterior view\", \"reverse angle\", \"backing\"],\n",
        "            \"left\": [\"left side\", \"left profile\", \"from the left\", \"port side view\", \"left aspect\", \"sinistral view\"],\n",
        "            \"right\": [\"right side\", \"right profile\", \"from the right\", \"starboard side view\", \"right aspect\", \"dextral view\"],\n",
        "            \"threeQuarterFrontLeft\": [\"angled front left\", \"partial front left\", \"diagonal front left\", \"oblique front left\", \"left forward angle\", \"left front perspective\"],\n",
        "            \"threeQuarterFrontRight\": [\"angled front right\", \"partial front right\", \"diagonal front right\", \"oblique front right\", \"right forward angle\", \"right front perspective\"],\n",
        "            \"threeQuarterBackLeft\": [\"angled back left\", \"partial back left\", \"diagonal back left\", \"oblique back left\", \"left rear angle\", \"left back perspective\"],\n",
        "            \"threeQuarterBackRight\": [\"angled back right\", \"partial back right\", \"diagonal back right\", \"oblique back right\", \"right rear angle\", \"right back perspective\"],\n",
        "            \"overhead\": [\"from above\", \"top view\", \"bird's perspective\", \"downward view\", \"superior view\", \"zenith angle\"],\n",
        "            \"silhouette\": [\"shadow form\", \"outlined shape\", \"backlit profile\", \"contour view\", \"rim lit\", \"shape outline\"]\n",
        "        }\n",
        "    },\n",
        "    \"CameraMovementType\": {\n",
        "        \"values\": [\n",
        "            \"static\", \"panLeft\", \"panRight\", \"tiltUp\", \"tiltDown\",\n",
        "            \"dollyIn\", \"dollyOut\", \"truckLeft\", \"truckRight\",\n",
        "            \"pedestalUp\", \"pedestalDown\", \"arcLeft\", \"arcRight\",\n",
        "            \"craneUp\", \"craneDown\", \"dollyOutZoomIn\", \"dollyInZoomOut\",\n",
        "            \"dutchLeft\", \"dutchRight\", \"follow\",\n",
        "            \"spiral\",\n",
        "            \"snakeTrack\",\n",
        "            \"boomerang\"\n",
        "        ],\n",
        "        \"synonyms\": {\n",
        "            \"static\": [\"stationary\", \"fixed\", \"still\", \"locked off\", \"immobile\", \"stable\"],\n",
        "            \"panLeft\": [\"pan left\", \"sweep left\", \"rotate left\", \"horizontal left\", \"left scan\", \"leftward pan\"],\n",
        "            \"panRight\": [\"pan right\", \"sweep right\", \"rotate right\", \"horizontal right\", \"right scan\", \"rightward pan\"],\n",
        "            \"tiltUp\": [\"tilt upward\", \"look up\", \"angle up\", \"vertical up\", \"upward pivot\", \"ascend view\"],\n",
        "            \"tiltDown\": [\"tilt downward\", \"look down\", \"angle down\", \"vertical down\", \"downward pivot\", \"descend view\"],\n",
        "            \"dollyIn\": [\"move forward\", \"push in\", \"track forward\", \"advance\", \"forward track\", \"approach\"],\n",
        "            \"dollyOut\": [\"move backward\", \"pull out\", \"track backward\", \"retreat\", \"backward track\", \"withdraw\"],\n",
        "            \"truckLeft\": [\"slide left\", \"track left\", \"lateral left\", \"crab left\", \"sideways left\", \"parallel left\"],\n",
        "            \"truckRight\": [\"slide right\", \"track right\", \"lateral right\", \"crab right\", \"sideways right\", \"parallel right\"],\n",
        "            \"pedestalUp\": [\"raise up\", \"elevate\", \"lift up\", \"vertical rise\", \"upward boost\", \"ascend\"],\n",
        "            \"pedestalDown\": [\"lower down\", \"descend\", \"move down\", \"vertical drop\", \"downward sink\", \"descend\"],\n",
        "            \"arcLeft\": [\"curve left\", \"orbit left\", \"circular left\", \"left circle\", \"rounded left\", \"left orbit\"],\n",
        "            \"arcRight\": [\"curve right\", \"orbit right\", \"circular right\", \"right circle\", \"rounded right\", \"right orbit\"],\n",
        "            \"craneUp\": [\"boom up\", \"jib up\", \"rise up\", \"sweep up\", \"ascending arc\", \"upward boom\"],\n",
        "            \"craneDown\": [\"boom down\", \"jib down\", \"lower\", \"sweep down\", \"descending arc\", \"downward boom\"],\n",
        "            \"dollyOutZoomIn\": [\"pull back and zoom\", \"compensating pullback\", \"contra-zoom out\", \"reverse dolly zoom\", \"backward zoom\", \"vertigo effect\"],\n",
        "            \"dollyInZoomOut\": [\"push in and zoom out\", \"compensating push\", \"contra-zoom in\", \"forward dolly zoom\", \"forward zoom\", \"inverse vertigo\"],\n",
        "            \"dutchLeft\": [\"roll left\", \"tilt left\", \"diagonal left\", \"left rotation\", \"left cant\", \"oblique left\"],\n",
        "            \"dutchRight\": [\"roll right\", \"tilt right\", \"diagonal right\", \"right rotation\", \"right cant\", \"oblique right\"],\n",
        "            \"follow\": [\"track subject\", \"maintain follow\", \"chase movement\", \"pursuit shot\", \"accompany motion\", \"shadow movement\"],\n",
        "            \"spiral\": [\"helical movement\", \"corkscrew motion\", \"spiral track\", \"circular descent\", \"winding path\", \"coil movement\"],\n",
        "            \"snakeTrack\": [\"serpentine movement\", \"winding track\", \"meandering motion\", \"curved path\", \"s-curve movement\", \"flowing track\"],\n",
        "            \"boomerang\": [\"return movement\", \"back-and-forth\", \"pendulum motion\", \"swing track\", \"reversing path\", \"loop movement\"]\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "48FpXYWMcCqA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Generate a Dataset with Random Number Parameter"
      ],
      "metadata": {
        "id": "tGsU22rpbdkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import json\n",
        "\n",
        "# Expanded template components with more natural variations\n",
        "TEMPLATE_COMPONENTS = {\n",
        "    \"angle\": [\n",
        "        \"from a {angle} angle\",\n",
        "        \"with a {angle} perspective\",\n",
        "        \"maintaining a {angle} view\",\n",
        "        \"using a {angle} vantage point\",\n",
        "        \"positioned at a {angle} level\",\n",
        "        \"set up with a {angle} viewpoint\",\n",
        "        \"utilizing a {angle} camera position\",\n",
        "        \"with the camera {angle}\",\n",
        "        \"capturing from {angle}\",\n",
        "        \"at a {angle} height\"\n",
        "    ],\n",
        "    \"shot_size\": [\n",
        "        \"capture a {shot_size}\",\n",
        "        \"frame a {shot_size}\",\n",
        "        \"execute a {shot_size}\",\n",
        "        \"create a {shot_size} shot\",\n",
        "        \"compose a {shot_size}\",\n",
        "        \"establish a {shot_size}\",\n",
        "        \"set up a {shot_size}\",\n",
        "        \"design a {shot_size} composition\",\n",
        "        \"deliver a {shot_size}\",\n",
        "        \"aim for a {shot_size}\"\n",
        "    ],\n",
        "    \"movement_type\":[\n",
        "        \"as the camera {movement_type}\",\n",
        "        \"with a {movement_type} movement\",\n",
        "        \"using a {movement_type} motion\",\n",
        "        \"implementing a {movement_type}\",\n",
        "        \"executing a {movement_type}\",\n",
        "        \"performing a {movement_type}\",\n",
        "        \"following through with a {movement_type}\",\n",
        "        \"{movement_type} the camera {movement_speed}\",\n",
        "        \"in a {movement_type} pattern\",\n",
        "        \"with camera movement {movement_type}\"\n",
        "    ],\n",
        "    \"movement_speed\": [\n",
        "        \"with speed {movement_speed}\",\n",
        "        \"at {movement_speed}\",\n",
        "        \"with a {movement_speed} pace\",\n",
        "        \"{movement_type} the camera\",\n",
        "        \"moving {movement_speed}\",\n",
        "        \"{movement_speed}\",\n",
        "        \"with a {movement_speed} speed\",\n",
        "        \"at a {movement_speed} rate\"\n",
        "    ],\n",
        "    \"frame_position\": [\n",
        "        \"with the subject positioned {frame_position}\",\n",
        "        \"keeping the subject {frame_position} in frame\",\n",
        "        \"placing the subject {frame_position}\",\n",
        "        \"maintaining the subject {frame_position}\",\n",
        "        \"featuring the subject {frame_position}\",\n",
        "        \"with subject placement {frame_position}\",\n",
        "        \"composing the subject {frame_position}\",\n",
        "        \"arranging the subject {frame_position}\",\n",
        "        \"positioning our focus {frame_position}\",\n",
        "        \"with the main element {frame_position}\"\n",
        "    ],\n",
        "    \"subject_view\": [\n",
        "        \"showing their {subject_view}\",\n",
        "        \"capturing their {subject_view}\",\n",
        "        \"emphasizing their {subject_view}\",\n",
        "        \"highlighting their {subject_view}\",\n",
        "        \"revealing their {subject_view}\",\n",
        "        \"displaying the {subject_view}\",\n",
        "        \"featuring their {subject_view}\",\n",
        "        \"presenting the {subject_view}\",\n",
        "        \"focusing on their {subject_view}\",\n",
        "        \"accentuating the {subject_view}\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Opening phrases to add variety\n",
        "OPENING_PHRASES = [\n",
        "    \"The shot requires\",\n",
        "    \"Set up\",\n",
        "    \"The scene calls for\",\n",
        "    \"We need\",\n",
        "    \"This shot demands\",\n",
        "    \"Let's capture\",\n",
        "    \"Plan to get\",\n",
        "    \"The sequence needs\",\n",
        "    \"We're looking for\",\n",
        "    \"Arrange\",\n",
        "    \"Position the camera to\",\n",
        "    \"The frame should\",\n",
        "    \"We want to\",\n",
        "    \"The goal is to\",\n",
        "    \"Focus on\"\n",
        "]\n",
        "\n",
        "# Connecting phrases for more natural flow\n",
        "CONNECTING_PHRASES = [\n",
        "    \"while\",\n",
        "    \"as\",\n",
        "    \"and\",\n",
        "    \", then\",\n",
        "    \". Also,\",\n",
        "    \". Meanwhile,\",\n",
        "    \", with\",\n",
        "    \". At the same time,\",\n",
        "    \". Additionally,\",\n",
        "    \", making sure to\"\n",
        "]\n",
        "\n",
        "def generate_dynamic_template(num_params):\n",
        "    \"\"\"Generate a more natural template with a specific number of parameters\"\"\"\n",
        "    param_mapping = {\n",
        "        \"CameraVerticalAngle\": \"angle\",\n",
        "        \"ShotSize\": \"shot_size\",\n",
        "        \"MovementSpeed\": \"movement_speed\",\n",
        "        \"CameraMovementType\": \"movement_type\",\n",
        "        \"SubjectInFramePosition\": \"frame_position\",\n",
        "        \"SubjectView\": \"subject_view\"\n",
        "    }\n",
        "\n",
        "    # Select random parameters\n",
        "    available_params = list(set(param_mapping.keys()))\n",
        "    selected_params = random.sample(available_params, min(num_params, len(available_params)))\n",
        "\n",
        "    # Build template with more natural language structure\n",
        "    template_parts = []\n",
        "    used_components = set()\n",
        "\n",
        "    # Randomly decide whether to use an opening phrase\n",
        "    if random.random() < 0.7:  # 70% chance to use opening phrase\n",
        "        template_parts.append(random.choice(OPENING_PHRASES))\n",
        "\n",
        "    # Generate component parts\n",
        "    component_parts = []\n",
        "    for param in selected_params:\n",
        "        component_key = param_mapping[param]\n",
        "        if component_key not in used_components:\n",
        "            component_parts.append(random.choice(TEMPLATE_COMPONENTS[component_key]))\n",
        "            used_components.add(component_key)\n",
        "\n",
        "    # Randomly arrange components with connecting phrases\n",
        "    while component_parts:\n",
        "        template_parts.append(component_parts.pop(random.randint(0, len(component_parts)-1)))\n",
        "        if component_parts and random.random() < 0.7:  # 70% chance to add connector\n",
        "            template_parts.append(random.choice(CONNECTING_PHRASES))\n",
        "\n",
        "    # Join all parts and clean up any double spaces or awkward punctuation\n",
        "    template = \" \".join(template_parts)\n",
        "    template = template.replace(\" ,\", \",\")\n",
        "    template = template.replace(\"  \", \" \")\n",
        "    template = template.strip()\n",
        "\n",
        "    # Ensure proper ending punctuation\n",
        "    if not template.endswith((\".\",\"!\")):\n",
        "        template += \".\"\n",
        "    return template, selected_params\n",
        "\n",
        "def generate_prompt(num_params):\n",
        "    \"\"\"Generate a single prompt with specified number of parameters\"\"\"\n",
        "    template, selected_params = generate_dynamic_template(num_params)\n",
        "\n",
        "    params = {}\n",
        "    original_params = {}\n",
        "\n",
        "    # Generate values only for selected parameters\n",
        "    for param_name in selected_params:\n",
        "        value = random.choice(CAMERA_PARAMETERS[param_name][\"values\"])\n",
        "        synonym = random.choice(CAMERA_PARAMETERS[param_name][\"synonyms\"][value])\n",
        "\n",
        "        params[param_name.lower()] = synonym\n",
        "        original_params[param_name] = value\n",
        "\n",
        "    # Prepare template parameters\n",
        "    template_params = {\n",
        "        \"angle\": params.get(\"cameraverticalangle\", \"\"),\n",
        "        \"shot_size\": params.get(\"shotsize\", \"\"),\n",
        "        \"movement_type\": params.get(\"cameramovementtype\", \"\"),\n",
        "        \"movement_speed\": params.get(\"movementspeed\", \"\"),\n",
        "        \"frame_position\": params.get(\"subjectinframeposition\", \"\"),\n",
        "        \"subject_view\": params.get(\"subjectview\", \"\")\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        prompt = template.format(**template_params)\n",
        "        # Clean up any potential double spaces\n",
        "        prompt = \" \".join(prompt.split())\n",
        "    except KeyError as e:\n",
        "        print(f\"Template error: {e}\")\n",
        "        print(f\"Template: {template}\")\n",
        "        print(f\"Params: {template_params}\")\n",
        "        return None\n",
        "\n",
        "    return {\n",
        "        \"prompt\": prompt,\n",
        "        \"parameters\": original_params,\n",
        "        \"template\": template\n",
        "    }\n",
        "\n",
        "def generate_dataset(num_samples=1000):\n",
        "    \"\"\"Generate a dataset of prompts with varying numbers of parameters\"\"\"\n",
        "    dataset = []\n",
        "    for _ in range(num_samples):\n",
        "        num_params = random.randint(2, 6)\n",
        "        entry = generate_prompt(num_params)\n",
        "        if entry:\n",
        "            dataset.append(entry)\n",
        "    return dataset\n",
        "\n",
        "# Generate and save the dataset\n",
        "if __name__ == \"__main__\":\n",
        "    dataset = generate_dataset()\n",
        "\n",
        "    with open(\"generated_variable_camera_prompts.json\", \"w\") as f:\n",
        "        json.dump(dataset, f, indent=2)\n",
        "\n",
        "    print(\"\\nExample generated prompts:\")\n",
        "    for i in range(min(3, len(dataset))):\n",
        "        print(f\"\\nPrompt {i+1}:\")\n",
        "        print(\"Text:\", dataset[i][\"prompt\"])\n",
        "        print(\"Parameters:\", dataset[i][\"parameters\"])\n",
        "        print(\"Template:\", dataset[i][\"template\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLENGMK9bc_r",
        "outputId": "ffcc6412-a56c-4bd3-a81e-f909ddcb120d"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example generated prompts:\n",
            "\n",
            "Prompt 1:\n",
            "Text: Plan to get composing the subject very top as the camera curved path . Also, showing their forward facing, making sure to create a complete view shot, then with a choreographed pauses pace while positioned at a from directly above level.\n",
            "Parameters: {'ShotSize': 'fullShot', 'SubjectView': 'front', 'SubjectInFramePosition': 'outerTop', 'MovementSpeed': 'deliberateStartStop', 'CameraMovementType': 'snakeTrack', 'CameraVerticalAngle': 'overhead'}\n",
            "Template: Plan to get composing the subject {frame_position} as the camera {movement_type} . Also, showing their {subject_view}, making sure to create a {shot_size} shot, then with a {movement_speed} pace while positioned at a {angle} level.\n",
            "\n",
            "Prompt 2:\n",
            "Text: The shot requires set up with a looking down viewpoint, with following through with a lateral right, making sure to composing the subject low right showing their left rear angle as establish a head to toe lateral right the camera.\n",
            "Parameters: {'ShotSize': 'fullShot', 'CameraMovementType': 'truckRight', 'SubjectInFramePosition': 'bottomRight', 'SubjectView': 'threeQuarterBackLeft', 'MovementSpeed': 'stopAndGo', 'CameraVerticalAngle': 'high'}\n",
            "Template: The shot requires set up with a {angle} viewpoint, with following through with a {movement_type}, making sure to composing the subject {frame_position} showing their {subject_view} as establish a {shot_size} {movement_type} the camera.\n",
            "\n",
            "Prompt 3:\n",
            "Text: Position the camera to with a unchanging pace pace . At the same time, in a right rotation pattern set up a head to toe as with a looking down perspective emphasizing their left aspect . Meanwhile, featuring the subject southeast position.\n",
            "Parameters: {'MovementSpeed': 'constant', 'CameraMovementType': 'dutchRight', 'SubjectView': 'left', 'ShotSize': 'fullShot', 'SubjectInFramePosition': 'bottomRight', 'CameraVerticalAngle': 'high'}\n",
            "Template: Position the camera to with a {movement_speed} pace . At the same time, in a {movement_type} pattern set up a {shot_size} as with a {angle} perspective emphasizing their {subject_view} . Meanwhile, featuring the subject {frame_position}.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[900]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7epYvCFPhxM0",
        "outputId": "d1703461-34c2-4685-dfab-8a1d0314df71"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'prompt': 'periodic pauses . Meanwhile, with camera movement winding track while using a neutral vantage point keeping the subject upper port in frame.',\n",
              " 'parameters': {'MovementSpeed': 'stopAndGo',\n",
              "  'SubjectInFramePosition': 'topLeft',\n",
              "  'CameraVerticalAngle': 'eye',\n",
              "  'CameraMovementType': 'snakeTrack'},\n",
              " 'template': '{movement_speed} . Meanwhile, with camera movement {movement_type} while using a {angle} vantage point keeping the subject {frame_position} in frame.'}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def analyze_template_parameters(dataset_path):\n",
        "    \"\"\"\n",
        "    Load a dataset generated by the template-based system and create binary arrays\n",
        "    indicating which parameters are present in each entry.\n",
        "\n",
        "    Args:\n",
        "        dataset_path (str): Path to the JSON dataset file\n",
        "\n",
        "    Returns:\n",
        "        list: List of binary arrays where 1 indicates parameter presence and 0 indicates absence\n",
        "    \"\"\"\n",
        "    # Define the ordered list of all possible parameters\n",
        "    all_parameters = [\n",
        "        \"CameraVerticalAngle\",\n",
        "        \"ShotSize\",\n",
        "        \"MovementSpeed\",\n",
        "        \"SubjectInFramePosition\",\n",
        "        \"SubjectView\",\n",
        "        \"CameraMovementType\"\n",
        "    ]\n",
        "\n",
        "    # Load the dataset\n",
        "    with open(dataset_path, 'r') as f:\n",
        "        dataset = json.load(f)\n",
        "\n",
        "    # Initialize the result list\n",
        "    binary_representations = []\n",
        "\n",
        "    # Process each entry in the dataset\n",
        "    for entry in dataset:\n",
        "        # Get the parameters present in this entry\n",
        "        present_parameters = entry['parameters'].keys()\n",
        "        # Create binary array for this entry\n",
        "        binary_array = [1 if param in present_parameters else 0 for param in all_parameters]\n",
        "        binary_representations.append(binary_array)\n",
        "\n",
        "    return binary_representations\n",
        "\n",
        "# Example usage and verification\n",
        "if __name__ == \"__main__\":\n",
        "    # Analyze the dataset\n",
        "    result = analyze_template_parameters(\"generated_variable_camera_prompts.json\")\n",
        "\n",
        "    # Print results with parameter names for verification\n",
        "    all_parameters = [\n",
        "        \"CameraVerticalAngle\",\n",
        "        \"ShotSize\",\n",
        "        \"MovementSpeed\",\n",
        "        \"SubjectInFramePosition\",\n",
        "        \"SubjectView\",\n",
        "        \"CameraMovementType\"\n",
        "    ]\n",
        "\n",
        "    print(f\"\\nAnalyzed {len(result)} entries\")\n",
        "    print(\"\\nExample entries:\")\n",
        "\n",
        "    # Print first 3 entries with detailed information\n",
        "    mask = []\n",
        "    for i, binary_array in enumerate(result[:3]):\n",
        "        print(f\"\\nEntry {i + 1}:\")\n",
        "        print(len(dataset[i]['parameters']))\n",
        "        print(\"Parameter Presence:\")\n",
        "        for param, present in zip(all_parameters, binary_array):\n",
        "            status = \"Present\" if present == 1 else \"Absent\"\n",
        "            print(f\"{param}: {status}\")\n",
        "        print(f\"Binary representation: {binary_array}\")\n",
        "        mask.append(binary_array)\n",
        "\n",
        "\n",
        "    # Print some statistics\n",
        "    param_counts = [sum(x) for x in zip(*result)]\n",
        "    print(\"\\nParameter usage statistics:\")\n",
        "    for param, count in zip(all_parameters, param_counts):\n",
        "        percentage = (count / len(result)) * 100\n",
        "        print(f\"{param}: used in {count} entries ({percentage:.1f}%)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZca7Eg5gpNF",
        "outputId": "af301a34-f4e1-40d8-abf1-55c25a242d82"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Analyzed 1000 entries\n",
            "\n",
            "Example entries:\n",
            "\n",
            "Entry 1:\n",
            "6\n",
            "Parameter Presence:\n",
            "CameraVerticalAngle: Present\n",
            "ShotSize: Present\n",
            "MovementSpeed: Present\n",
            "SubjectInFramePosition: Present\n",
            "SubjectView: Present\n",
            "CameraMovementType: Present\n",
            "Binary representation: [1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Entry 2:\n",
            "6\n",
            "Parameter Presence:\n",
            "CameraVerticalAngle: Present\n",
            "ShotSize: Present\n",
            "MovementSpeed: Present\n",
            "SubjectInFramePosition: Present\n",
            "SubjectView: Present\n",
            "CameraMovementType: Present\n",
            "Binary representation: [1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Entry 3:\n",
            "6\n",
            "Parameter Presence:\n",
            "CameraVerticalAngle: Present\n",
            "ShotSize: Present\n",
            "MovementSpeed: Present\n",
            "SubjectInFramePosition: Present\n",
            "SubjectView: Present\n",
            "CameraMovementType: Present\n",
            "Binary representation: [1, 1, 1, 1, 1, 1]\n",
            "\n",
            "Parameter usage statistics:\n",
            "CameraVerticalAngle: used in 688 entries (68.8%)\n",
            "ShotSize: used in 670 entries (67.0%)\n",
            "MovementSpeed: used in 644 entries (64.4%)\n",
            "SubjectInFramePosition: used in 700 entries (70.0%)\n",
            "SubjectView: used in 667 entries (66.7%)\n",
            "CameraMovementType: used in 659 entries (65.9%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Training (BERT)"
      ],
      "metadata": {
        "id": "D5d-0EbEWD2m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define all possible parameter keys and values\n",
        "parameter_keys = {\n",
        "    \"CameraVerticalAngle\": [\"low\", \"eye\", \"high\", \"overhead\", \"birdsEye\", \"Not_Specified\"],\n",
        "    \"ShotSize\": [\n",
        "        \"extremeCloseUp\",\n",
        "            \"closeUp\",\n",
        "            \"mediumCloseUp\",\n",
        "            \"mediumShot\",\n",
        "            \"fullShot\",\n",
        "            \"longShot\",\n",
        "            \"veryLongShot\",\n",
        "            \"extremeLongShot\"\n",
        "       , \"Not_Specified\"],\n",
        "    # \"MovementSpeed\": [\n",
        "    #     \"slowToFast\",\n",
        "    #         \"fastToSlow\",\n",
        "    #         \"constant\",\n",
        "    #         \"stopAndGo\",\n",
        "    #         \"deliberateStartStop\",\n",
        "    #         \"erratic\",\n",
        "    #         \"pulsing\",\n",
        "    #     \"Not_Specified\"],\n",
        "    # \"SubjectInFramePosition\": [\n",
        "    #      \"left\", \"right\", \"top\", \"bottom\", \"center\",\n",
        "    #         \"topLeft\", \"topRight\", \"bottomLeft\", \"bottomRight\",\n",
        "    #         \"outerLeft\", \"outerRight\", \"outerTop\", \"outerBottom\",\n",
        "    #         \"offsetCenter\",\n",
        "    #         \"diagonal\",\n",
        "    #     \"Not_Specified\"],\n",
        "    # \"SubjectView\": [\n",
        "    #     \"front\",\n",
        "    #         \"back\",\n",
        "    #         \"left\",\n",
        "    #         \"right\",\n",
        "    #         \"threeQuarterFrontLeft\",\n",
        "    #         \"threeQuarterFrontRight\",\n",
        "    #         \"threeQuarterBackLeft\",\n",
        "    #         \"threeQuarterBackRight\",\n",
        "    #         \"overhead\",\n",
        "    #         \"silhouette\",\n",
        "    #     \"Not_Specified\"],\n",
        "    # \"CameraMovementType\": [\n",
        "    #     \"static\", \"panLeft\", \"panRight\", \"tiltUp\", \"tiltDown\",\n",
        "    #         \"dollyIn\", \"dollyOut\", \"truckLeft\", \"truckRight\",\n",
        "    #         \"pedestalUp\", \"pedestalDown\", \"arcLeft\", \"arcRight\",\n",
        "    #         \"craneUp\", \"craneDown\", \"dollyOutZoomIn\", \"dollyInZoomOut\",\n",
        "    #         \"dutchLeft\", \"dutchRight\", \"follow\",\n",
        "    #         \"spiral\",\n",
        "    #         \"snakeTrack\",\n",
        "    #         \"boomerang\",\n",
        "    #     \"Not_Specified\"]\n",
        "}\n"
      ],
      "metadata": {
        "id": "Dw4E9R-5tmbz"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ParameterEncoder:\n",
        "    def __init__(self, parameter_keys):\n",
        "        self.parameter_keys = parameter_keys\n",
        "        self.num_classes = sum(len(values) for values in parameter_keys.values())\n",
        "\n",
        "    def encode(self, parameters):\n",
        "        encoded = []\n",
        "        for key, values in self.parameter_keys.items():\n",
        "            class_index = values.index(parameters.get(key, \"Not_Specified\"))\n",
        "            encoded.append((key, class_index))  # Store (key, class_index) pairs\n",
        "        return encoded\n",
        "\n",
        "    def decode(self, encoded_labels):\n",
        "        decoded_params = {}\n",
        "        for key, class_index in encoded_labels:\n",
        "            decoded_params[key] = self.parameter_keys[key][class_index]\n",
        "        return decoded_params"
      ],
      "metadata": {
        "id": "L_pwNvtV1qQ4"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "with open(\"generated_variable_camera_prompts.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Extract prompts and parameters\n",
        "prompts = [item[\"prompt\"] for item in dataset]\n",
        "parameters = [item[\"parameters\"] for item in dataset]\n",
        "\n",
        "# Instantiate the encoder\n",
        "encoder = ParameterEncoder(parameter_keys)\n",
        "\n",
        "# Encode parameters using the new encoder\n",
        "encoded_parameters = [encoder.encode(p) for p in parameters]\n",
        "print(len(encoded_parameters))\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(prompts, encoded_parameters, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzUehNFymIoR",
        "outputId": "2233fa05-eda5-4f26-db8e-c927f5c5d85c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7hEEykPl8uB",
        "outputId": "480c7a3d-d904-4bbd-ea8f-bd9d41170022"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('CameraVerticalAngle', 2), ('ShotSize', 0)]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CameraDataset(Dataset):\n",
        "    def __init__(self, prompts, labels, tokenizer, parameter_keys, max_length=128):\n",
        "        self.prompts = prompts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.parameter_keys = parameter_keys  # Pass parameter_keys to the dataset\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prompts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt = self.prompts[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(prompt, max_length=self.max_length, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "        # Create a one-hot encoded label vector for each parameter\n",
        "        label_vector = []\n",
        "        for key, values in self.parameter_keys.items():\n",
        "            one_hot = [0] * len(values)\n",
        "            for param_key, class_index in label:\n",
        "                if param_key == key:\n",
        "                    one_hot[class_index] = 1\n",
        "                    break  # Move to the next parameter after finding a match\n",
        "            label_vector.extend(one_hot)\n",
        "\n",
        "        return encoding[\"input_ids\"].squeeze(0), encoding[\"attention_mask\"].squeeze(0), torch.tensor(label_vector, dtype=torch.float)\n",
        "\n",
        "\n",
        "# Initialize tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "bert_model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = CameraDataset(X_train, y_train, tokenizer, parameter_keys)\n",
        "test_dataset = CameraDataset(X_test, y_test, tokenizer, parameter_keys)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Define the model\n",
        "class CameraPredictor(nn.Module):\n",
        "    def __init__(self, bert_model, parameter_keys):\n",
        "        super(CameraPredictor, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.feature_classifiers = nn.ModuleDict()  # Dictionary to hold feature-specific MLPs\n",
        "\n",
        "        for key, values in parameter_keys.items():\n",
        "            self.feature_classifiers[key] = nn.Sequential(\n",
        "                nn.Linear(bert_model.config.hidden_size, 64),  # Hidden layer\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, len(values))  # Output layer\n",
        "            )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        x = self.dropout(pooled_output)\n",
        "\n",
        "        feature_outputs = {}\n",
        "        for key, classifier in self.feature_classifiers.items():\n",
        "            feature_outputs[key] = classifier(x) # Apply softmax for single-value prediction\n",
        "        return feature_outputs\n",
        "\n",
        "# Instantiate the model\n",
        "model = CameraPredictor(bert_model, parameter_keys)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Loss and Optimizer (modified for multi-task learning)\n",
        "criterion = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for single class\n",
        "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
        "\n",
        "\n",
        "def calculate_accuracy(outputs, labels):\n",
        "    total_correct = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    for key in outputs:\n",
        "        feature_outputs = outputs[key]\n",
        "        start_index = sum(len(parameter_keys[k]) for k in parameter_keys if k < key)\n",
        "        end_index = start_index + len(parameter_keys[key])\n",
        "        feature_labels = labels[:, start_index:end_index]\n",
        "        _, predicted_indices = torch.max(feature_outputs, dim=1)  # Get predicted indices\n",
        "        _, true_indices = torch.max(feature_labels, dim=1)  # Get true indices\n",
        "\n",
        "        correct_predictions = (predicted_indices == true_indices).float()\n",
        "        total_correct += correct_predictions.sum().item()\n",
        "        total_predictions += correct_predictions.numel()\n",
        "\n",
        "    accuracy = total_correct / total_predictions if total_predictions else 0\n",
        "    return accuracy\n",
        "\n",
        "def calculate_loss(outputs, labels):\n",
        "    total_loss = 0\n",
        "    for key, values in parameter_keys.items():\n",
        "        start_index = sum(len(parameter_keys[k]) for k in parameter_keys if k < key)\n",
        "        end_index = start_index + len(values)\n",
        "        feature_labels = labels[:, start_index:end_index].to(device)  # Extract labels for the feature\n",
        "        class_indices = torch.argmax(feature_labels , dim= 1)\n",
        "        feature_outputs = outputs[key]  # Get output for the feature\n",
        "        loss = criterion(feature_outputs, feature_labels)\n",
        "        total_loss += loss\n",
        "    return total_loss\n",
        "\n",
        "def train_model(model, train_loader, test_loader, epochs=5):\n",
        "    best_val_accuracy = 0\n",
        "    best_model_state_dict = None  # Store the state dict of the best model\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        train_accuracy = 0\n",
        "        num_train_batches = 0\n",
        "\n",
        "        for input_ids, attention_mask, labels in train_loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            total_loss = calculate_loss(outputs, labels)\n",
        "            total_loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            train_accuracy += calculate_accuracy(outputs, labels)\n",
        "            train_loss += total_loss.item()\n",
        "            num_train_batches += 1\n",
        "\n",
        "        avg_train_loss = train_loss / num_train_batches\n",
        "        avg_train_accuracy = train_accuracy / num_train_batches\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        test_accuracy = 0\n",
        "        num_test_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for input_ids, attention_mask, labels in test_loader:\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(input_ids, attention_mask)\n",
        "                total_loss = calculate_loss(outputs, labels)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                test_accuracy += calculate_accuracy(outputs, labels)\n",
        "                test_loss += total_loss.item()\n",
        "                num_test_batches += 1\n",
        "\n",
        "        avg_test_loss = test_loss / num_test_batches\n",
        "        avg_test_accuracy = test_accuracy / num_test_batches\n",
        "\n",
        "        # Track best validation accuracy\n",
        "        if avg_test_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = avg_test_accuracy\n",
        "            best_model_state_dict = model.state_dict()  # Save the model's state dict\n",
        "\n",
        "\n",
        "        print(f\"Epoch {epoch+1}\")\n",
        "        print(f\"Training Loss: {avg_train_loss:.4f}, Training Accuracy: {avg_train_accuracy:.4f}\")\n",
        "        print(f\"Validation Loss: {avg_test_loss:.4f}, Validation Accuracy: {avg_test_accuracy:.4f}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # Load the best model after training\n",
        "    if best_model_state_dict is not None:\n",
        "        model.load_state_dict(best_model_state_dict)  # Load the best model's state dict\n",
        "        print(\"Loaded best model with accuracy:\", best_val_accuracy)\n",
        "\n",
        "# Train the model and evaluate\n",
        "train_model(model, train_loader, test_loader, epochs=100)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 946
        },
        "id": "VtavftmAIoKI",
        "outputId": "ad36fefc-bd49-4213-df84-d0fee6a508d5"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "Training Loss: 3.7511, Training Accuracy: 0.3113\n",
            "Validation Loss: 3.3839, Validation Accuracy: 0.3389\n",
            "--------------------------------------------------\n",
            "Epoch 2\n",
            "Training Loss: 3.3106, Training Accuracy: 0.3538\n",
            "Validation Loss: 3.1298, Validation Accuracy: 0.3822\n",
            "--------------------------------------------------\n",
            "Epoch 3\n",
            "Training Loss: 3.1280, Training Accuracy: 0.4019\n",
            "Validation Loss: 3.0194, Validation Accuracy: 0.3726\n",
            "--------------------------------------------------\n",
            "Epoch 4\n",
            "Training Loss: 3.1296, Training Accuracy: 0.3831\n",
            "Validation Loss: 3.1621, Validation Accuracy: 0.3389\n",
            "--------------------------------------------------\n",
            "Epoch 5\n",
            "Training Loss: 3.5236, Training Accuracy: 0.3225\n",
            "Validation Loss: 3.9225, Validation Accuracy: 0.3245\n",
            "--------------------------------------------------\n",
            "Epoch 6\n",
            "Training Loss: 3.8221, Training Accuracy: 0.3144\n",
            "Validation Loss: 3.8176, Validation Accuracy: 0.3245\n",
            "--------------------------------------------------\n",
            "Epoch 7\n",
            "Training Loss: 3.7705, Training Accuracy: 0.3212\n",
            "Validation Loss: 3.7918, Validation Accuracy: 0.3245\n",
            "--------------------------------------------------\n",
            "Epoch 8\n",
            "Training Loss: 3.7319, Training Accuracy: 0.3187\n",
            "Validation Loss: 3.7323, Validation Accuracy: 0.3245\n",
            "--------------------------------------------------\n",
            "Epoch 9\n",
            "Training Loss: 3.6242, Training Accuracy: 0.3125\n",
            "Validation Loss: 3.4528, Validation Accuracy: 0.3245\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-10e5fb3e5117>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;31m# Train the model and evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-10e5fb3e5117>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, test_loader, epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;31m# Calculate training accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcalculate_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtotal_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mnum_train_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-40-10e5fb3e5117>\u001b[0m in \u001b[0;36mcalculate_accuracy\u001b[0;34m(outputs, labels)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mcorrect_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted_indices\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrue_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mtotal_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcorrect_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0mtotal_predictions\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcorrect_predictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.nn import functional as F\n",
        "\n",
        "def predict(prompts, labels, model, parameter_keys, encoder, return_raw=False):\n",
        "\n",
        "    model.eval()\n",
        "    all_predicted_params = []\n",
        "    all_raw_outputs = []\n",
        "    total_loss = 0\n",
        "    total_accuracy = 0\n",
        "    num_prompts = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for i, prompt in enumerate(prompts):\n",
        "        encoded_label = encoder.encode(labels[i])\n",
        "\n",
        "        encoding = tokenizer(prompt, max_length=128, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "        input_ids = encoding[\"input_ids\"].to(device)\n",
        "        attention_mask = encoding[\"attention_mask\"].to(device)\n",
        "\n",
        "        outputs = model(input_ids, attention_mask)\n",
        "        raw_outputs = {key: output.cpu().numpy() for key, output in outputs.items()}\n",
        "        if return_raw:\n",
        "            all_raw_outputs.append(raw_outputs)\n",
        "        else:\n",
        "            predicted_params = {}\n",
        "            start_idx = 0\n",
        "            for key, values in parameter_keys.items():\n",
        "                predicted_value_idx = np.argmax(raw_outputs[key][0])\n",
        "                predicted_params[key] = values[predicted_value_idx]\n",
        "            all_predicted_params.append(predicted_params)\n",
        "\n",
        "        # Prepare the label_vector in one-hot encoded format from the encoder\n",
        "        label_vector = []\n",
        "        for key, values in parameter_keys.items():\n",
        "            one_hot = [0] * len(values)\n",
        "            for param_key, class_index in encoded_label:\n",
        "                if key == param_key:\n",
        "                    one_hot[class_index] = 1\n",
        "                else:\n",
        "                    one_hot[values.index(\"Not_Specified\")] = 1\n",
        "            label_vector.extend(one_hot)\n",
        "\n",
        "        label_tensor = torch.tensor(label_vector, dtype=torch.float).to(device)\n",
        "\n",
        "        # Compute loss and accuracy using the provided functions\n",
        "        batch_loss = calculate_loss(outputs, torch.unsqueeze(label_tensor,0))\n",
        "        batch_accuracy = calculate_accuracy(outputs, torch.unsqueeze(label_tensor,0))\n",
        "        total_loss += batch_loss.item()\n",
        "        total_accuracy += batch_accuracy\n",
        "        num_prompts +=1\n",
        "\n",
        "    avg_loss = total_loss / num_prompts if num_prompts > 0 else 0\n",
        "    avg_accuracy = total_accuracy / num_prompts if num_prompts > 0 else 0\n",
        "\n",
        "    return all_predicted_params if not return_raw else None, all_raw_outputs, avg_accuracy, avg_loss"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "trt2IFEccAxF"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "source": [
        "# Assuming model, tokenizer, parameter_keys, device, encoder, calculate_loss and calculate_accuracy are already defined\n",
        "prompts_to_predict = [\n",
        "    \"The shot requires a low-angle perspective and a close-up.\",\n",
        "    \"Set up a medium shot with the subject on the right.\",\n",
        "    \"The scene calls for the camera to pan left and use a full shot.\",\n",
        "    \"We need a very long shot of the subject with a pulsing speed.\"\n",
        "]\n",
        "\n",
        "# Dummy labels for demonstration, replace with your actual labels\n",
        "labels_to_predict = [\n",
        "    {\"CameraVerticalAngle\": \"low\", \"ShotSize\": \"closeUp\"},\n",
        "    {\"ShotSize\": \"mediumShot\", \"SubjectInFramePosition\": \"right\"},\n",
        "    {\"CameraMovementType\": \"panLeft\", \"ShotSize\": \"fullShot\"},\n",
        "    {\"ShotSize\": \"veryLongShot\", \"MovementSpeed\": \"pulsing\"}\n",
        "]\n",
        "\n",
        "\n",
        "predicted_params, raw_outputs, accuracy, loss = predict(prompts_to_predict, labels_to_predict, model, parameter_keys, encoder)\n",
        "\n",
        "if predicted_params:\n",
        "    for i, prompt in enumerate(prompts_to_predict):\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "        print(f\"Predicted Parameters: {predicted_params[i]}\")\n",
        "        print(f\"labels to predict: {labels_to_predict[i]}\")\n",
        "\n",
        "print(f\"Overall Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Overall Loss: {loss:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "NIWsG1fmcDz6",
        "outputId": "e1dbe0ce-1628-4b73-fc27-083843ecd36b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompt: The shot requires a low-angle perspective and a close-up.\n",
            "Predicted Parameters: {'CameraVerticalAngle': 'low', 'ShotSize': 'extremeCloseUp', 'MovementSpeed': 'erratic', 'SubjectInFramePosition': 'top', 'SubjectView': 'Not_Specified', 'CameraMovementType': 'craneDown'}\n",
            "labels to predict: {'CameraVerticalAngle': 'low', 'ShotSize': 'closeUp'}\n",
            "Prompt: Set up a medium shot with the subject on the right.\n",
            "Predicted Parameters: {'CameraVerticalAngle': 'low', 'ShotSize': 'extremeCloseUp', 'MovementSpeed': 'erratic', 'SubjectInFramePosition': 'top', 'SubjectView': 'Not_Specified', 'CameraMovementType': 'craneDown'}\n",
            "labels to predict: {'ShotSize': 'mediumShot', 'SubjectInFramePosition': 'right'}\n",
            "Prompt: The scene calls for the camera to pan left and use a full shot.\n",
            "Predicted Parameters: {'CameraVerticalAngle': 'low', 'ShotSize': 'extremeCloseUp', 'MovementSpeed': 'erratic', 'SubjectInFramePosition': 'top', 'SubjectView': 'Not_Specified', 'CameraMovementType': 'craneDown'}\n",
            "labels to predict: {'CameraMovementType': 'panLeft', 'ShotSize': 'fullShot'}\n",
            "Prompt: We need a very long shot of the subject with a pulsing speed.\n",
            "Predicted Parameters: {'CameraVerticalAngle': 'low', 'ShotSize': 'extremeCloseUp', 'MovementSpeed': 'erratic', 'SubjectInFramePosition': 'top', 'SubjectView': 'Not_Specified', 'CameraMovementType': 'craneDown'}\n",
            "labels to predict: {'ShotSize': 'veryLongShot', 'MovementSpeed': 'pulsing'}\n",
            "Overall Accuracy: 0.6667\n",
            "Overall Loss: 16.1874\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation on LLM generated Prompt"
      ],
      "metadata": {
        "id": "xd7oZLh30TGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "with open(\"focused_camera_prompts_dataset.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Extract prompts and parameters\n",
        "prompts = [item[\"prompt\"] for item in dataset]\n",
        "parameters = [item[\"parameters\"] for item in dataset]\n",
        "\n",
        "def standardize_parameters(params):\n",
        "    \"\"\"\n",
        "    Add 'Not_Specified' for any missing parameters from the set of 6 standard parameters.\n",
        "    \"\"\"\n",
        "    all_parameters = [\n",
        "        \"CameraVerticalAngle\",\n",
        "        \"ShotSize\",\n",
        "        \"MovementSpeed\",\n",
        "        \"SubjectInFramePosition\",\n",
        "        \"SubjectView\",\n",
        "        \"CameraMovementType\"\n",
        "    ]\n",
        "\n",
        "    standardized_params = params.copy()\n",
        "    for param in all_parameters:\n",
        "        if param not in standardized_params:\n",
        "            standardized_params[param] = \"Not_Specified\"\n",
        "\n",
        "    return standardized_params\n",
        "parameters = np.array([standardize_parameters(p) for p in parameters])\n"
      ],
      "metadata": {
        "id": "FhM7jsbl0SPo",
        "outputId": "311f7a23-636e-4ee9-c61a-bdba66017517",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'focused_camera_prompts_dataset.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ca93a90fa3da>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"focused_camera_prompts_dataset.json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Extract prompts and parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'focused_camera_prompts_dataset.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(prompts)):\n",
        "    predicted = predict( prompts[i])\n",
        "    print(\"Prompt:\",  prompts[i])\n",
        "    print(\"Predicted Parameters:\", predicted)\n",
        "    print(\"Actual Parameters:\", parameters[i])\n",
        "    print(\" \\n\")"
      ],
      "metadata": {
        "id": "5snflhM42aVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_accuracy(prompts, parameters, predict_fn):\n",
        "    \"\"\"\n",
        "    Compute accuracy for parameter predictions.\n",
        "    \"\"\"\n",
        "    all_parameters = [\n",
        "        \"CameraVerticalAngle\",\n",
        "        \"ShotSize\",\n",
        "        \"MovementSpeed\",\n",
        "        \"SubjectInFramePosition\",\n",
        "        \"SubjectView\",\n",
        "        \"CameraMovementType\"\n",
        "    ]\n",
        "\n",
        "    # Initialize counters for each parameter\n",
        "    param_correct = {param: 0 for param in all_parameters}\n",
        "    param_total = {param: 0 for param in all_parameters}\n",
        "    total_correct = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    # Track mismatches for error analysis\n",
        "    mismatches = []\n",
        "\n",
        "    for i in range(len(prompts)):\n",
        "        example_prompt = prompts[i]\n",
        "        predicted = predict_fn(example_prompt)\n",
        "        actual = standardize_parameters(parameters[i])\n",
        "\n",
        "        # Track mismatches for this prompt\n",
        "        prompt_mismatches = []\n",
        "\n",
        "        # Check each parameter\n",
        "        for param in all_parameters:\n",
        "            param_total[param] += 1\n",
        "            total_predictions += 1\n",
        "\n",
        "            pred_value = predicted.get(param, \"Not_Specified\")\n",
        "            actual_value = actual.get(param, \"Not_Specified\")\n",
        "\n",
        "            if pred_value == actual_value:\n",
        "                param_correct[param] += 1\n",
        "                total_correct += 1\n",
        "            else:\n",
        "                prompt_mismatches.append({\n",
        "                    'parameter': param,\n",
        "                    'predicted': pred_value,\n",
        "                    'actual': actual_value\n",
        "                })\n",
        "\n",
        "        if prompt_mismatches:\n",
        "            mismatches.append({\n",
        "                'prompt': example_prompt,\n",
        "                'mismatches': prompt_mismatches\n",
        "            })\n",
        "\n",
        "    # Calculate accuracies\n",
        "    overall_accuracy = total_correct / total_predictions\n",
        "    param_accuracies = {param: param_correct[param] / param_total[param]\n",
        "                       for param in all_parameters}\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nOverall Accuracy:\", f\"{overall_accuracy:.4f}\")\n",
        "    print(\"\\nPer-Parameter Accuracies:\")\n",
        "    for param, acc in param_accuracies.items():\n",
        "        print(f\"{param}: {acc:.4f}\")\n",
        "\n",
        "    # Print example mismatches\n",
        "    print(\"\\nExample Mismatches (first 5):\")\n",
        "    for mismatch in mismatches[:5]:\n",
        "        print(f\"\\nPrompt: {mismatch['prompt']}\")\n",
        "        print(\"Mismatched Parameters:\")\n",
        "        for error in mismatch['mismatches']:\n",
        "            print(f\"- {error['parameter']}: predicted '{error['predicted']}' instead of '{error['actual']}'\")\n",
        "\n",
        "    return {\n",
        "        'overall_accuracy': overall_accuracy,\n",
        "        'parameter_accuracies': param_accuracies,\n",
        "        'mismatches': mismatches\n",
        "    }\n",
        "\n",
        "# Use the function\n",
        "results = compute_accuracy(prompts, parameters, predict)"
      ],
      "metadata": {
        "id": "PCReL_KE9B_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Generation (Using LLM)"
      ],
      "metadata": {
        "id": "pV92-mAhWGHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install --upgrade openai\n"
      ],
      "metadata": {
        "id": "rnj6b8RI1QDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tYT2Y7Bd0JCi"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import random\n",
        "import json\n",
        "\n",
        "# Initialize OpenAI client with your API key\n",
        "client = OpenAI(api_key=\"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\")\n",
        "\n",
        "# Parameters for generating prompts\n",
        "parameters = {\n",
        "    \"CameraVerticalAngle\": [\"low\", \"eye\", \"high\", \"overhead\", \"birdsEye\"],\n",
        "    \"ShotSize\": [\n",
        "        \"extremeCloseUp\",\n",
        "        \"closeUp\",\n",
        "        \"mediumCloseUp\",\n",
        "        \"mediumShot\",\n",
        "        \"fullShot\",\n",
        "        \"longShot\",\n",
        "        \"veryLongShot\",\n",
        "        \"extremeLongShot\",\n",
        "    ],\n",
        "    \"MovementSpeed\": [\n",
        "        \"slowToFast\",\n",
        "        \"fastToSlow\",\n",
        "        \"constant\",\n",
        "        \"stopAndGo\",\n",
        "        \"deliberateStartStop\",\n",
        "    ],\n",
        "    \"SubjectInFramePosition\": [\n",
        "        \"left\",\n",
        "        \"right\",\n",
        "        \"top\",\n",
        "        \"bottom\",\n",
        "        \"center\",\n",
        "        \"topLeft\",\n",
        "        \"topRight\",\n",
        "        \"bottomLeft\",\n",
        "        \"bottomRight\",\n",
        "        \"outerLeft\",\n",
        "        \"outerRight\",\n",
        "        \"outerTop\",\n",
        "        \"outerBottom\",\n",
        "    ],\n",
        "    \"SubjectView\": [\n",
        "        \"front\",\n",
        "        \"back\",\n",
        "        \"left\",\n",
        "        \"right\",\n",
        "        \"threeQuarterFrontLeft\",\n",
        "        \"threeQuarterFrontRight\",\n",
        "        \"threeQuarterBackLeft\",\n",
        "        \"threeQuarterBackRight\",\n",
        "    ],\n",
        "    \"CameraMovementType\": [\n",
        "        \"static\",\n",
        "        \"panLeft\",\n",
        "        \"panRight\",\n",
        "        \"tiltUp\",\n",
        "        \"tiltDown\",\n",
        "        \"dollyIn\",\n",
        "        \"dollyOut\",\n",
        "        \"truckLeft\",\n",
        "        \"truckRight\",\n",
        "        \"pedestalUp\",\n",
        "        \"pedestalDown\",\n",
        "        \"arcLeft\",\n",
        "        \"arcRight\",\n",
        "        \"craneUp\",\n",
        "        \"craneDown\",\n",
        "        \"dollyOutZoomIn\",\n",
        "        \"dollyInZoomOut\",\n",
        "        \"dutchLeft\",\n",
        "        \"dutchRight\",\n",
        "        \"follow\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "# Function to interact with OpenAI API\n",
        "def chat_gpt(prompt):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",  # Use the gpt-4o-mini model\n",
        "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "    )\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "# Function to generate a single diverse user prompt\n",
        "def generate_single_prompt():\n",
        "    input_params = {\n",
        "        \"CameraVerticalAngle\": random.choice(parameters[\"CameraVerticalAngle\"]),\n",
        "        \"ShotSize\": random.choice(parameters[\"ShotSize\"]),\n",
        "        \"MovementSpeed\": random.choice(parameters[\"MovementSpeed\"]),\n",
        "        \"SubjectInFramePosition\": random.choice(parameters[\"SubjectInFramePosition\"]),\n",
        "        \"SubjectView\": random.choice(parameters[\"SubjectView\"]),\n",
        "        \"CameraMovementType\": random.choice(parameters[\"CameraMovementType\"]),\n",
        "    }\n",
        "\n",
        "    # Allow random omission of some parameters\n",
        "    params_to_include = random.sample(list(input_params.keys()), random.randint(3, len(input_params)))\n",
        "    filtered_params = {key: value for key, value in input_params.items() if key in params_to_include}\n",
        "\n",
        "    # Create a dynamic and focused prompt\n",
        "    prompt = (\n",
        "        f\"You are describing a camera setup and movement. Focus only on the camera's movements, \"\n",
        "        f\"angles, framing, and motion. Here are the camera \"\n",
        "        f\"parameters to consider:\\n\\n\"\n",
        "        + \"\\n\".join([f\"- {key.replace('Camera', '').replace('InFrame', ' In Frame')} is {value}\" for key, value in filtered_params.items()])\n",
        "        + \"\\n\\n\"\n",
        "        f\"Please describe the shot naturally and realistically, using varied and human-like expressions. Be concise. \"\n",
        "    )\n",
        "\n",
        "    #print (f\"Prompt: \\n {prompt} \\n\")\n",
        "\n",
        "    # Call GPT to generate a prompt\n",
        "    user_prompt = chat_gpt(prompt)\n",
        "\n",
        "    #print (f\"Output: \\n {user_prompt} \\n\")\n",
        "    return {\"prompt\": user_prompt, \"parameters\": filtered_params}\n",
        "\n",
        "# Generate a dataset\n",
        "def generate_dataset(num_samples=100):\n",
        "    dataset = []\n",
        "    for _ in range(num_samples):\n",
        "        dataset.append(generate_single_prompt())\n",
        "    return dataset\n",
        "\n",
        "# Save the dataset\n",
        "dataset = generate_dataset()\n",
        "with open(\"focused_camera_prompts_dataset.json\", \"w\") as f:\n",
        "    json.dump(dataset, f, indent=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0].keys()"
      ],
      "metadata": {
        "id": "WCub9GZ4YN1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def analyze_parameters(dataset_path):\n",
        "    \"\"\"\n",
        "    Load a dataset and create binary arrays indicating which parameters are present in each entry.\n",
        "\n",
        "    Args:\n",
        "        dataset_path (str): Path to the JSON dataset file\n",
        "\n",
        "    Returns:\n",
        "        list: List of binary arrays where 1 indicates parameter presence and 0 indicates absence\n",
        "    \"\"\"\n",
        "    # Define the ordered list of all possible parameters\n",
        "    all_parameters = [\n",
        "        \"CameraVerticalAngle\",\n",
        "        \"ShotSize\",\n",
        "        \"MovementSpeed\",\n",
        "        \"SubjectInFramePosition\",\n",
        "        \"SubjectView\",\n",
        "        \"CameraMovementType\"\n",
        "    ]\n",
        "\n",
        "    # Load the dataset\n",
        "    with open(dataset_path, 'r') as f:\n",
        "        dataset = json.load(f)\n",
        "\n",
        "    # Initialize the result list\n",
        "    binary_representations = []\n",
        "\n",
        "    # Process each entry in the dataset\n",
        "    for entry in dataset:\n",
        "        # Get the parameters present in this entry\n",
        "        present_parameters = entry['parameters'].keys()\n",
        "\n",
        "        # Create binary array for this entry\n",
        "        binary_array = [1 if param in present_parameters else 0 for param in all_parameters]\n",
        "        binary_representations.append(binary_array)\n",
        "\n",
        "    return binary_representations\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Analyze the dataset\n",
        "    result = analyze_parameters(\"focused_camera_prompts_dataset.json\")\n",
        "\n",
        "    # Print results with parameter names for verification\n",
        "    all_parameters = [\n",
        "        \"CameraVerticalAngle\",\n",
        "        \"ShotSize\",\n",
        "        \"MovementSpeed\",\n",
        "        \"SubjectInFramePosition\",\n",
        "        \"SubjectView\",\n",
        "        \"CameraMovementType\"\n",
        "    ]\n",
        "    mask = []\n",
        "    for i, binary_array in enumerate(result):\n",
        "        print(f\"\\nEntry {i + 1}:\")\n",
        "        print(len(dataset[i]['parameters']))\n",
        "        print(\"Parameter Presence:\")\n",
        "        for param, present in zip(all_parameters, binary_array):\n",
        "            status = \"Present\" if present == 1 else \"Absent\"\n",
        "            print(f\"{param}: {status}\")\n",
        "        print(f\"Binary representation: {binary_array}\")\n",
        "        mask.append(binary_array)"
      ],
      "metadata": {
        "id": "Hln-KO9JUytH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask"
      ],
      "metadata": {
        "id": "Jg1WZTArZTi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Traning (CLIP)"
      ],
      "metadata": {
        "id": "-Uh89mIWldDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "id": "gxKQca2hleod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import clip\n",
        "import json\n",
        "import numpy as np\n",
        "from transformers import CLIPTokenizer\n",
        "\n",
        "# Load the dataset\n",
        "with open(\"generated_camera_prompts.json\", \"r\") as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Extract prompts and parameters\n",
        "prompts = [item[\"prompt\"] for item in dataset]\n",
        "parameters = [item[\"parameters\"] for item in dataset]\n",
        "\n",
        "# Define all possible parameter keys and values (same as before)\n",
        "parameter_keys = {\n",
        "    \"CameraVerticalAngle\": [\"low\", \"eye\", \"high\", \"overhead\", \"birdsEye\"],\n",
        "    \"ShotSize\": [\n",
        "        \"extremeCloseUp\", \"closeUp\", \"mediumCloseUp\", \"mediumShot\",\n",
        "        \"fullShot\", \"longShot\", \"veryLongShot\", \"extremeLongShot\",\n",
        "    ],\n",
        "    \"MovementSpeed\": [\n",
        "        \"slowToFast\", \"fastToSlow\", \"constant\", \"stopAndGo\",\n",
        "        \"deliberateStartStop\",\n",
        "    ],\n",
        "    \"SubjectInFramePosition\": [\n",
        "        \"left\", \"right\", \"top\", \"bottom\", \"center\", \"topLeft\", \"topRight\",\n",
        "        \"bottomLeft\", \"bottomRight\", \"outerLeft\", \"outerRight\", \"outerTop\",\n",
        "        \"outerBottom\",\n",
        "    ],\n",
        "    \"SubjectView\": [\n",
        "        \"front\", \"back\", \"left\", \"right\", \"threeQuarterFrontLeft\",\n",
        "        \"threeQuarterFrontRight\", \"threeQuarterBackLeft\", \"threeQuarterBackRight\",\n",
        "    ],\n",
        "    \"CameraMovementType\": [\n",
        "        \"static\", \"panLeft\", \"panRight\", \"tiltUp\", \"tiltDown\", \"dollyIn\",\n",
        "        \"dollyOut\", \"truckLeft\", \"truckRight\", \"pedestalUp\", \"pedestalDown\",\n",
        "        \"arcLeft\", \"arcRight\", \"craneUp\", \"craneDown\", \"dollyOutZoomIn\",\n",
        "        \"dollyInZoomOut\", \"dutchLeft\", \"dutchRight\", \"follow\",\n",
        "    ],\n",
        "}\n",
        "\n",
        "# One-hot encode parameters (same as before)\n",
        "def encode_parameters(parameters):\n",
        "    encoded = []\n",
        "    for key, values in parameter_keys.items():\n",
        "        vec = [0] * len(values)\n",
        "        if key in parameters and parameters[key] in values:\n",
        "            vec[values.index(parameters[key])] = 1\n",
        "        encoded.extend(vec)\n",
        "    return encoded\n",
        "\n",
        "encoded_parameters = np.array([encode_parameters(p) for p in parameters])\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    prompts, encoded_parameters, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Define a custom dataset for CLIP\n",
        "class CLIPCameraDataset(Dataset):\n",
        "    def __init__(self, prompts, labels, tokenizer, max_length=77):  # CLIP's max token length is 77\n",
        "        self.prompts = prompts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.prompts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        prompt = self.prompts[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Tokenize with truncation and padding\n",
        "        encoding = self.tokenizer(\n",
        "            prompt,\n",
        "            max_length=self.max_length,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        return (\n",
        "            encoding.input_ids.squeeze(0),\n",
        "            encoding.attention_mask.squeeze(0),\n",
        "            torch.tensor(label, dtype=torch.float)\n",
        "        )\n",
        "\n",
        "# Initialize CLIP model and tokenizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = CLIPCameraDataset(X_train, y_train, tokenizer)\n",
        "test_dataset = CLIPCameraDataset(X_test, y_test, tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "# Define the model using CLIP's text encoder\n",
        "class CLIPCameraPredictor(nn.Module):\n",
        "    def __init__(self, clip_model, num_labels):\n",
        "        super(CLIPCameraPredictor, self).__init__()\n",
        "        self.clip = clip_model\n",
        "        self.text_projection = clip_model.text_projection\n",
        "\n",
        "        # Freeze CLIP parameters\n",
        "        for param in self.clip.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # New layers for parameter prediction\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "        self.fc1 = nn.Linear(512, 256)  # CLIP's text embedding dimension is 512\n",
        "        self.fc2 = nn.Linear(256, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        # Get CLIP text embeddings\n",
        "        text_features = self.clip.encode_text(input_ids)\n",
        "        text_features = text_features / text_features.norm(dim=1, keepdim=True)\n",
        "\n",
        "        # Process through our layers\n",
        "        x = self.dropout(text_features)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.sigmoid(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "num_labels = y_train.shape[1]\n",
        "model = CLIPCameraPredictor(clip_model, num_labels)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.AdamW(\n",
        "    [p for p in model.parameters() if p.requires_grad],\n",
        "    lr=1e-4\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "def train_model(model, train_loader, test_loader, epochs=5):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for input_ids, attention_mask, labels in train_loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Loss: {train_loss / len(train_loader):.4f}\")\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for input_ids, attention_mask, labels in test_loader:\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(input_ids, attention_mask)\n",
        "                loss = criterion(outputs, labels)\n",
        "                test_loss += loss.item()\n",
        "        print(f\"Validation Loss: {test_loss / len(test_loader):.4f}\")\n",
        "\n",
        "# Predict function\n",
        "def predict(prompt):\n",
        "    model.eval()\n",
        "    encoding = tokenizer(\n",
        "        prompt,\n",
        "        max_length=77,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    input_ids = encoding.input_ids.to(device)\n",
        "    attention_mask = encoding.attention_mask.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, attention_mask).cpu().numpy()[0]\n",
        "\n",
        "    predicted_params = {}\n",
        "    start_idx = 0\n",
        "    for key, values in parameter_keys.items():\n",
        "        end_idx = start_idx + len(values)\n",
        "        predicted_value_idx = np.argmax(outputs[start_idx:end_idx])\n",
        "        if outputs[start_idx:end_idx][predicted_value_idx] > 0.1:\n",
        "            predicted_params[key] = values[predicted_value_idx]\n",
        "        start_idx = end_idx\n",
        "    return predicted_params\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, test_loader, epochs=50)\n",
        "\n",
        "# Test prediction\n",
        "example_prompt = X_test[0]\n",
        "predicted = predict(example_prompt)\n",
        "print(\"Prompt:\", example_prompt)\n",
        "print(\"Predicted Parameters:\", predicted)\n",
        "print(\"Actual Parameters:\", y_test[0])"
      ],
      "metadata": {
        "id": "R3RkP_9VU3mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "with torch.no_grad():\n",
        "    for batch_X, batch_y in test_loader:\n",
        "        outputs = torch.sigmoid(model(batch_X)) > 0.5\n",
        "        correct += (outputs.numpy() == batch_y.numpy()).sum()\n",
        "        total += batch_y.numel()\n",
        "\n",
        "print(f\"Accuracy: {correct / total:.2%}\")\n"
      ],
      "metadata": {
        "id": "aGhbqyJ-ItJW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}